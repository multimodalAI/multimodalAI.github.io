<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.b9231f75dcc97a371ce6141b4d2aadaf.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><script async src="https://www.googletagmanager.com/gtag/js?id=G-5K2K2GHWHQ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","G-5K2K2GHWHQ",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><meta name=author content="Haiping Lu"><meta name=description content><link rel=alternate hreflang=en-us href=https://multimodalAI.github.io/home/><link rel=canonical href=https://multimodalAI.github.io/home/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu48a82c8450dfe9aca8259bacb1bf2d41_406640_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu48a82c8450dfe9aca8259bacb1bf2d41_406640_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@MultimodalAI_UK"><meta property="twitter:creator" content="@MultimodalAI_UK"><meta property="twitter:image" content="https://multimodalAI.github.io/media/icon_hu48a82c8450dfe9aca8259bacb1bf2d41_406640_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Multimodal AI Community UK"><meta property="og:url" content="https://multimodalAI.github.io/home/"><meta property="og:title" content="Multimodal AI Community UK"><meta property="og:description" content><meta property="og:image" content="https://multimodalAI.github.io/media/icon_hu48a82c8450dfe9aca8259bacb1bf2d41_406640_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><title>Multimodal AI Community UK</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=8e7bc052bdfc6746ea2bb6595e8093eb><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Multimodal AI Community UK</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Multimodal AI Community UK</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Latest Event</span></a></li><li class=nav-item><a class=nav-link href=/#speaker><span>Keynote Speaker</span></a></li><li class=nav-item><a class=nav-link href=/#programme><span>Programme</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Past Events</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/multimodalaisprint23><span>MultimodalAISprint'23</span></a>
<a class=dropdown-item href=/multimodalai23><span>MultimodalAI'23</span></a></div></li><li class=nav-item><a class=nav-link href=/opening><span>Openings</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=slider class="home-section wg-slider carousel slide" data-ride=carousel data-interval=6000><div class=home-section-bg></div><ol class=carousel-indicators><li data-target=#slider data-slide-to=0 class=active></li><li data-target=#slider data-slide-to=1></li><li data-target=#slider data-slide-to=2></li></ol><div class=carousel-inner><div class="carousel-item active fullscreen" style=background-color:#666;background-image:url(https://multimodalAI.github.io/media/slide.jpg);background-repeat:no-repeat;background-position:100%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.6);backdrop-filter:brightness(.6)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:center><h1 class=hero-title>First Multimodal AI Community Forum</h1><p class=hero-lead style="margin:0 auto"><p><font size=3 style=color:#fff!important>An online AI UK Fringe event - Monday 25th March 2024</font><div style=text-align:center><a href=https://docs.google.com/forms/d/e/1FAIpQLSd1nknP8OirbYGXloUw-911l4XMzHTvT0CagugQvItpAMgVfQ/viewform class=btn style=margin:5px;background-color:#fff!important;color:purple!important><i class="fas fa-user"></i> Register by 14th March</a><br></p></div></p></div></div></div><div class="carousel-item fullscreen" style=background-color:#666;background-image:url(https://multimodalAI.github.io/media/slide.jpg);background-repeat:no-repeat;background-position:100%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.6);backdrop-filter:brightness(.6)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:center><h1 class=hero-title>Current Openings</h1><p class=hero-lead style="margin:0 auto"><p><font size=3 style=color:#fff!important>Find or post PhD / job openings in multimodal AI</font><div style=text-align:center><a href=opening class=btn style=margin:5px;background-color:#fff!important;color:purple!important><i class="fas fa-star"></i> PhD / Job Openings</a><br></p></div></p></div></div></div><div class="carousel-item fullscreen" style=background-color:#666;background-image:url(https://multimodalAI.github.io/media/slide.jpg);background-repeat:no-repeat;background-position:100%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.6);backdrop-filter:brightness(.6)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:center><h1 class=hero-title>Second Workshop on Multimodal AI</h1><p class=hero-lead style="margin:0 auto"><p><font size=3 style=color:#fff!important>25th June 2024, Sheffield, UK</font><div style=text-align:center><a href=https://twitter.com/MultimodalAI_UK class=btn style=margin:5px;background-color:#fff!important;color:purple!important><i class="fab fa-twitter"></i> Follow Us</a></p></div></p></div></div></div></div><a class=carousel-control-prev href=#slider data-slide=prev><span class=carousel-control-prev-icon></span>
<span class=sr-only>Previous</span>
</a><a class=carousel-control-next href=#slider data-slide=next><span class=carousel-control-next-icon></span>
<span class=sr-only>Next</span></a></section><section id=temp_top class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class=col-12><p><strong>Upcoming Event(s)</strong></p><ul><li>The Second Workshop on Multimodal AI，25th June 2024，Sheffield, UK.</li></ul></div></div></div></section><section id=about class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class=col-12><p>Welcome to &ldquo;<strong>The First Multimodal AI Community Forum</strong>&rdquo;, an online <a href=https://ai-uk.turing.ac.uk/fringe-events/ target=_blank rel=noopener>AI UK Fringe</a> event. This event is scheduled from <strong>13:00 to 17:00 (GMT) on Monday, 25th March 2024</strong>. The deadline for <a href=https://forms.gle/yckNWD8kHY5Z1wjo9 target=_blank rel=noopener>registration</a> is <strong>Thursday, 14th March 2024, 23:59 (GMT)</strong>.</p><p>Multimodal AI, which integrates various data modalities such as text, image, sound, and others, is swiftly revolutionising our interaction with technology and data. In our recent Turing Interest Group event (22nd Nov), &ldquo;<a href=https://multimodalai.github.io/multimodalaisprint23/ target=_blank rel=noopener>The First Multimodal AI Research Sprint</a>&rdquo;, we explored the diverse research states and methodologies in Multimodal AI across six areas and initiated the writing of a perspective paper on multimodal AI. Based on such past activities, this online forum aims to further bring together community members, from researchers to practitioners, to share their latest interdisciplinary perspectives and pioneering work in Multimodal AI. Our goal is to facilitate the exchange of fresh insights and foster connections and research progress within the Multimodal AI community.</p><p>We welcome researchers, practitioners, and students engaged in or interested in Multimodal AI from anywhere in the world to join us online. We also encourage the organisation of local community gatherings to watch and discuss the forum together.</p></div></div></div></section><section id=speaker class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Keynote Speaker</h1></div><div class="col-12 col-sm-auto people-person"><a href=/author/chunyuan-li/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/chunyuan-li/avatar_huad485b908f71d2416790700550d1daf8_243834_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/chunyuan-li/>Chunyuan Li</a></h2><h3>Research Lead at ByteDance/TikTok, (Co-)Lead Developer of LLaVA, Former Principal Researcher at Microsoft Research, Redmond</h3></div></div></div></div></section><section id=programme class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Final Programme</h1><p class=mt-1>Monday, 25th March 2024</p></div><div class=col-12><center><table><thead><tr><th>Time (GMT)</th><th>    Event</th></tr></thead><tbody><tr><td>13:00 - 13:10</td><td>    Opening</td></tr><tr><td></td><td>    Haiping Lu - Welcome and Introduction to the Multimodal AI Community</td></tr><tr><td>13:10 - 15:40</td><td>    Pitches</td></tr><tr><td></td><td>    <strong>Open Source Software</strong></td></tr><tr><td></td><td>        Florence Townend - Fusilli: a Python library for comparing deep fusion models</td></tr><tr><td></td><td>        Ana Lawry Aguila - multi-view-AE: a Python library of multi-view autoencoder methods</td></tr><tr><td></td><td>    <strong>Healthcare and Medicine</strong></td></tr><tr><td></td><td>        Jinge Wu - Introduction to Multimodal AI for Healthcare and Medicine</td></tr><tr><td></td><td>        Peter Charlton - Using Multimodal AI to Identify Determinants of Health and Wellbeing</td></tr><tr><td></td><td>        Halimat Afolabi - Med-Image Bind</td></tr><tr><td></td><td>        Dheeraj Giri - AI in Healthcare Needs Multimodal AI for Precision</td></tr><tr><td></td><td>        Farzana Patel - Using Multimodal AI Models to Support Clinical Diagnosis and Assessments</td></tr><tr><td></td><td>        Jinge Wu - Vision-Language Model in Radiology</td></tr><tr><td></td><td>        Jiachen Luo - Multimodal Emotion Recognition, Healthcare</td></tr><tr><td></td><td>        Kerf Tan - Multimodal Consolidation of Health Data, Breaking the Silos, Fast</td></tr><tr><td></td><td>        Anthony Hughes - Causal Graph Discovery Using Multimodal Models</td></tr><tr><td></td><td>    <strong>Break</strong></td></tr><tr><td></td><td>    <strong>Social Science and Humanities</strong></td></tr><tr><td></td><td>        Cyndie Demeocq - Introduction to Multimodal AI for Social Science and Humanities</td></tr><tr><td></td><td>        Shashank Shekhar - Unlock Relationships in Multi-Modal Data Using Knowledge Graphs</td></tr><tr><td></td><td>        Shitong Sun - Compositional Multimodal Learning</td></tr><tr><td></td><td>        Masoumeh Chapariniya - Computational Analysis of Personal Identity in Interaction, Recognition and Ethics</td></tr><tr><td></td><td>        Teodora Vukovic - VIAN-DH - Software for Multimodal Corpora Created by Liri In Zurich</td></tr><tr><td></td><td>        Ann Van De Velde - Scientific Illustration and Banned Words in Midjourney and DALLE</td></tr><tr><td></td><td>        Lucia Cipolina-Kun - The Reading of the Herculaneum Papyri Using AI</td></tr><tr><td></td><td>        Shan Wikoon - Agency in Building Near-Human Multimodal AI Teacher</td></tr><tr><td></td><td>    <strong>Break</strong></td></tr><tr><td></td><td>    <strong>Engineering</strong></td></tr><tr><td></td><td>        Xianyuan Liu - Introduction to Multimodal AI for Engineering</td></tr><tr><td></td><td>        Ehsan Nowroozi - Validating the Robustness of Cybersecurity Models</td></tr><tr><td></td><td>        Chao Zhang - Multimodal Learning in Embodied Applications</td></tr><tr><td></td><td>        Yao Zhang - AI in Control for Maritime Engineering</td></tr><tr><td></td><td>    <strong>Science</strong></td></tr><tr><td></td><td>        Yuhan Wang - Introduction to Multimodal AI for Science</td></tr><tr><td></td><td>        Ashwath Shetty - Alt-Text Generation for the Images Using Multimodal LLM</td></tr><tr><td></td><td>        Sogol Haghighat - Empowering AI Solutions: HPC Access, Research, and Consulting for Multimodal Models</td></tr><tr><td></td><td>    <strong>Break</strong></td></tr><tr><td></td><td>    <strong>Environment and Sustainability</strong></td></tr><tr><td></td><td>        Nataliya Tkachenko - Introduction to Multimodal AI for Environment and Sustainability</td></tr><tr><td></td><td>        Sachin Gaur - Approaches to Understand and Reduce Search Space for Video Data</td></tr><tr><td></td><td>        Natalia Efremova - AI for Agriculture</td></tr><tr><td></td><td>    <strong>Finance and Economics</strong></td></tr><tr><td></td><td>        Arunav Das - Introduction to Multimodal AI for Finance and Economics</td></tr><tr><td></td><td>        Yan Ge - Multimodal Multi-Task Asset Pricing with Token-Level Numeral Learning</td></tr><tr><td></td><td>    <strong>Break</strong></td></tr><tr><td>15:40 - 16:20</td><td>    Keynote Presentation by Chunyuan Li</td></tr><tr><td><td style=word-wrap:break-word;max-width:300px;text-align:justify;padding-left:1.3em;text-indent:-1.3em>    <strong>Title:</strong> <a href=https://llava-vl.github.io/ target=_blank rel=noopener>LLaVA: A Vision-Language Approach to Computer Vision in the Wild</a><br><strong>Abstract:</strong> The future of AI is in creating systems like foundation models that are pre-trained once, and will handle countless many downstream tasks directly (zero-shot), or adapt to new tasks quickly (few-shot). In this talk, I will discuss our vision-language approach to achieving “Computer Vision in the Wild (CVinW)”: building such a transferable system in computer vision (CV) that can effortlessly generalize to a wide range of visual recognition tasks in the wild. I will dive into Large Language-and-Vision Assistant (LLaVA) and its series, which is the first open-source project to exhibit the GPT-V level capabilities in image understanding and reasoning. I will demonstrate a promising path to build customizable large multimodal models that follow humans&rsquo; intent with an affordable cost.</td></td><td></td></tr><tr><td>16:20 - 17:00</td><td>    Open Discussion and Conclusions</td></tr></tbody></table></center></div></div></div></section><section id=contact class="home-section wg-contact"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Contact Us</h1></div><div class=col-12><ul class=fa-ul><li><i class="fa-li fas fa-envelope fa-2x" aria-hidden=true></i>
<a href=mailto:multimodal-ai-event-organisers-group@sheffield.ac.uk><a href=mailto:multimodal-ai-event-organisers-group@sheffield.ac.uk>multimodal-ai-event-organisers-group@sheffield.ac.uk</a></a></li></ul></div></div></div></section><section id=acknowledgement class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Acknowledgement</h1></div><div class=col-12><p>This event is brought to you by the Turing Interest Group on <a href=https://www.turing.ac.uk/research/interest-groups/meta-learning-multimodal-data target=_blank rel=noopener>Meta-Learning for Multimodal Data</a> (welcome to <a href="https://forms.office.com/Pages/ResponsePage.aspx?id=p_SVQ1XklU-Knx-672OE-fR6PcyyBV1JuragBENwKPJUOFhHNkY5WU1RVlczMjNWUVdYTDFDME1VNSQlQCN0PWcu" target=_blank rel=noopener>sign up</a>) and the Multimodal AI Community (welcome to <a href="https://groups.google.com/a/sheffield.ac.uk/g/multimodal-ai-community-group?pli=1" target=_blank rel=noopener>subscribe to our Google Group</a>) supported by the <a href=https://www.sheffield.ac.uk/machine-intelligence target=_blank rel=noopener>Centre for Machine Intelligence</a> at the University of Sheffield.</p><div style=text-align:center><img src=/media/AIUK_logo.png alt="AIUK Logo" style=width:200px;height:auto;display:inline-block;margin-right:20px>
<img src=/media/CMI_logo_1.png alt="CMI Logo" style=width:420px;height:auto;display:inline-block;margin-right:20px></div></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 Multimodal AI Community UK.</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.6ab16275cbca742a586c1726e3d94093.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script></body></html>