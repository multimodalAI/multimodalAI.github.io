<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.b9231f75dcc97a371ce6141b4d2aadaf.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><script async src="https://www.googletagmanager.com/gtag/js?id=G-5K2K2GHWHQ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","G-5K2K2GHWHQ",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><meta name=author content="Haiping Lu"><meta name=description content><link rel=alternate hreflang=en-us href=https://multimodalAI.github.io/home/><link rel=canonical href=https://multimodalAI.github.io/home/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu977d6aad2033a3e6559441911323f321_196795_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu977d6aad2033a3e6559441911323f321_196795_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@MultimodalAI_UK"><meta property="twitter:creator" content="@MultimodalAI_UK"><meta property="twitter:image" content="https://multimodalAI.github.io/media/icon_hu977d6aad2033a3e6559441911323f321_196795_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Multimodal AI Community UK"><meta property="og:url" content="https://multimodalAI.github.io/home/"><meta property="og:title" content="Multimodal AI Community UK"><meta property="og:description" content><meta property="og:image" content="https://multimodalAI.github.io/media/icon_hu977d6aad2033a3e6559441911323f321_196795_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><title>Multimodal AI Community UK</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=8e7bc052bdfc6746ea2bb6595e8093eb><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Multimodal AI Community UK</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Multimodal AI Community UK</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#speaker><span>Keynote Speaker</span></a></li><li class=nav-item><a class=nav-link href=/#about><span>Latest Event</span></a></li><li class=nav-item><a class=nav-link href=/#programme><span>Programme</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact Us</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Past Events</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/multimodalai-forum24><span>MultimodalAIForum'24</span></a>
<a class=dropdown-item href=/multimodalai-sprint23><span>MultimodalAISprint'23</span></a>
<a class=dropdown-item href=/multimodalai23><span>MultimodalAI'23</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=slider class="home-section wg-slider carousel slide" data-ride=carousel data-interval=8000><div class=home-section-bg></div><ol class=carousel-indicators><li data-target=#slider data-slide-to=0 class=active></li><li data-target=#slider data-slide-to=1></li><li data-target=#slider data-slide-to=2></li><li data-target=#slider data-slide-to=3></li></ol><div class=carousel-inner><div class="carousel-item active fullscreen" style=background-color:#666;background-image:url(https://multimodalAI.github.io/media/slide.jpg);background-repeat:no-repeat;background-position:100%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.6);backdrop-filter:brightness(.6)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:center><h1 class=hero-title>Second Workshop on Multimodal AI</h1><p class=hero-lead style="margin:0 auto"><p><font size=3 style=color:#fff!important>25th June 2024, Sheffield, UK</font></p><div style=text-align:center><a href=https://twitter.com/MultimodalAI_UK class=btn style=margin:5px;background-color:#fff!important;color:purple!important><i class="fab fa-twitter"></i> Follow Us</a></div></p></div></div></div><div class="carousel-item fullscreen" style=background-color:#666;background-image:url(https://multimodalAI.github.io/media/slide.jpg);background-repeat:no-repeat;background-position:100%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.6);backdrop-filter:brightness(.6)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:center><h1 class=hero-title>First Multimodal AI Community Forum</h1><p class=hero-lead style="margin:0 auto"><p><font size=3 style=color:#fff!important>25th March 2024, Virtual</font></p><div style=text-align:center><a href=https://multimodalai.github.io/multimodalai-forum24/ class=btn style=margin:5px;background-color:#fff!important;color:purple!important><i class="fas fa-history"></i> Review Event</a></div></p></div></div></div><div class="carousel-item fullscreen" style=background-color:#666;background-image:url(https://multimodalAI.github.io/media/slide.jpg);background-repeat:no-repeat;background-position:100%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.6);backdrop-filter:brightness(.6)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:center><h1 class=hero-title>First Multimodal AI Research Sprint</h1><p class=hero-lead style="margin:0 auto"><p><font size=3 style=color:#fff!important>22nd November 2023, London, UK</font></p><div style=text-align:center><a href=https://multimodalai.github.io/multimodalai-sprint23/ class=btn style=margin:5px;background-color:#fff!important;color:purple!important><i class="fas fa-history"></i> Review Event</a></div></p></div></div></div><div class="carousel-item fullscreen" style=background-color:#666;background-image:url(https://multimodalAI.github.io/media/slide.jpg);background-repeat:no-repeat;background-position:100%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.6);backdrop-filter:brightness(.6)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:center><h1 class=hero-title>First Workshop on Multimodal AI</h1><p class=hero-lead style="margin:0 auto"><p><font size=3 style=color:#fff!important>27th June 2023, Sheffield, UK</font></p><div style=text-align:center><a href=https://multimodalai.github.io/multimodalai23/ class=btn style=margin:5px;background-color:#fff!important;color:purple!important><i class="fas fa-history"></i> Review Event</a></div></p></div></div></div></div><a class=carousel-control-prev href=#slider data-slide=prev><span class=carousel-control-prev-icon></span>
<span class=sr-only>Previous</span>
</a><a class=carousel-control-next href=#slider data-slide=next><span class=carousel-control-next-icon></span>
<span class=sr-only>Next</span></a></section><section id=speaker class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Keynote Speakers</h1></div><div class="col-12 col-sm-auto people-person"><a href=/author/daniel-zugner/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/daniel-zugner/avatar_hu91256bf649c9679be078b2aca2d8500d_25070_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/daniel-zugner/>Daniel Zügner</a></h2><h3>Senior Researcher, Microsoft Research AI4Science</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/maria-liakata/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/maria-liakata/avatar_hu9c2ce7372631f90301dca34b5316e4f5_23009_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/maria-liakata/>Maria Liakata</a></h2><h3>Professor of Natural Language Processing, Queen Mary University of London & Turing AI Fellow</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/nataliya-tkachenko/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/nataliya-tkachenko/avatar_hud305836d2c327d7a6e62f9dc732663c7_174213_270x270_fill_lanczos_center_3.png alt=Avatar></a><div class=portrait-title><h2><a href=/author/nataliya-tkachenko/>Nataliya Tkachenko</a></h2><h3>Generative AI Ethics & Assurance Lead, Lloyds Banking Group</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/adam-steventon/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/adam-steventon/avatar_huc60c8463ae50687ab974ef3725cbd6b1_17860_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/adam-steventon/>Adam Steventon</a></h2><h3>Director of Data Platforms, Our Future Health</h3></div></div></div></div></section><section id=about class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class=col-12><p>You are warmly invited to join the Second Workshop on Multimodal AI (MultimodalAI'24), on 25th June 2024 at the Edge, University of Sheffield, UK. This workshop will be an in-person-only event. Pre-workshop activities, including a networking reception, will be organised on 24th June at Regent Court and The Diamond, University of Sheffield.</p><p>Multimodal AI, which integrates various data modalities such as text, image, sound, and others, is swiftly revolutionising our interaction with technology and data. MultimodalAI’24 brings together researchers and practitioners from AI, data science, and various scientific and application domains to discuss problems and challenges, share experiences and solutions, explore collaborations and future directions, and build networks and a vibrant community on multimodal AI.</p><p>This workshop features four keynote speakers from academia and industry: <a href="https://www.linkedin.com/in/maria-liakata-273b9677/?originalSubdomain=uk" target=_blank rel=noopener>Maria Liakata</a> (Professor of NLP, Queen Mary University of London, Turing AI Fellow), <a href=https://www.linkedin.com/in/danielzuegner/ target=_blank rel=noopener>Daniel Zügner</a> (Senior Researcher, Microsoft Research AI4Science), <a href="https://www.linkedin.com/in/nataliya-tkachenko-phd-b5ab8324/?originalSubdomain=uk" target=_blank rel=noopener>Nataliya Tkachenko</a> (Generative AI Ethics & Assurance Lead, Lloyds Banking Group), and <a href=https://www.linkedin.com/in/adam-steventon-864a2066/ target=_blank rel=noopener>Adam Steventon</a> (Director of Data Platforms, Our Future Health). We offer participants opportunities to present 5-minute talks and posters, with four prizes (£150 each) in total for the best talks and posters. We also have funds to support travel costs.</p><p><strong>Welcome to share the workshop flyer in <a href=../media/MultimodalAI2024.pdf>PDF</a> and <a href=../media/MultimodalAI2024.png>PNG</a> with your network.</strong></p></div></div></div></section><section id=pre-events class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Pre-workshop Events</h1><p>24th June 2024</p></div><div class=col-md-12><center><table><thead><tr><th>Time</th><th>    Event</th></tr></thead><tbody><tr><td>14:00 - 17:00</td><td>    Domain-specific meetings</td></tr><tr><td>17:00 - 19:00</td><td>    Networking reception</td></tr></tbody></table></center></div></div></div></section><section id=programme class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Final Programme (In Person Only)</h1><p>25th June 2024</p></div><div class=col-md-12><center><table><thead><tr><th>Time</th><th>    Event</th></tr></thead><tbody><tr><td>09:00 - 09:30</td><td>    Registration, morning refreshments, and poster session 1</td></tr><tr><td>09:30 - 09:35</td><td>    Welcome - <a href=/author/guy-brown/>Guy Brown</a>, Deputy Director of Centre for Machine Intelligence, University of Sheffield</td></tr><tr><td>09:35 - 10:00</td><td>    Introduction: exploring multimodal AI beyond vision and language, Haiping Lu</td></tr><tr><td>10:00 - 10:40</td><td>    Keynote 1: Daniel Zügner, Microsoft Research AI4Science</td></tr><tr><td><td style=word-wrap:break-word;max-width:810px;text-align:justify;padding-left:1.3em;text-indent:-1.3em>    <strong>Title:</strong> MatterGen: a generative model for inorganic materials design<br><strong>Abstract:</strong> The design of functional materials with desired properties is essential in driving technological advances in areas like energy storage, catalysis, and carbon capture. Traditionally, materials design is achieved by screening a large database of known materials and filtering down candidates based on the application. Generative models provide a new paradigm for materials design by directly generating entirely novel materials given desired property constraints. In this talk, we present MatterGen, a generative model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. To enable this, we introduce a new diffusion-based generative process that produces crystalline structures by gradually refining atom types, coordinates, and the periodic lattice. We further introduce adapter modules to enable fine-tuning towards any given property constraints with a labeled dataset. Compared to prior generative models, structures produced by MatterGen are more than twice as likely to be novel and stable, and more than 15 times closer to the local energy minimum. After fine-tuning, MatterGen successfully generates stable, novel materials with desired chemistry, symmetry, as well as mechanical, electronic and magnetic properties. Finally, we demonstrate multi-property materials design capabilities by proposing structures that have both high magnetic density and a chemical composition with low supply-chain risk. We believe that the quality of generated materials and the breadth of MatterGen&rsquo;s capabilities represent a major advancement towards creating a universal generative model for materials design.</td></td><td></td></tr><tr><td>10:40 - 11:20</td><td>    Community talks</td></tr><tr><td>11:20 - 11:50</td><td>    Break and poster session 2</td></tr><tr><td>11:50 - 12:30</td><td>    Keynote 2: Maria Liakata, Queen Mary University of London</td></tr><tr><td><td style=word-wrap:break-word;max-width:810px;text-align:justify;padding-left:1.3em;text-indent:-1.3em>    <strong>Title:</strong> Longitudinal language processing for dementia<br><strong>Abstract:</strong> While the advent of Large Language Modes (LLMs) has brought great promise to the field of AI there are many unresolved challenges especially around appropriate generation, temporal robustness, temporal and other reasoning and privacy concerns especially when working with sensitive content such as mental health data. The programme of work I have been leading consists in three core research directions: (1) data representation and generation (2) methods for personalised longitudinal models and temporal understanding (3) evaluation in real-world settings, with a focus on mental health. I will give an overview of work within my group on these topics and focus on work on longitudinal monitoring for dementia.</td></td><td></td></tr><tr><td>12:30 - 12:40</td><td>    Group Photos</td></tr><tr><td>12:40 - 14:00</td><td>    Lunch and poster session 3</td></tr><tr><td>14:00 - 14:40</td><td>    Keynote 3: Nataliya Tkachenko, Lloyds Banking Group</td></tr><tr><td><td style=word-wrap:break-word;max-width:810px;text-align:justify;padding-left:1.3em;text-indent:-1.3em>    <strong>Title:</strong> Ethical challenges for multimodal conversational banking & parametric insurance<br><strong>Abstract:</strong> Ever since mass-propagation of generative AI models, multimodal data has been getting increased attention from the customer-focused industries. Multimodal chatbots, which can process and respond to customer queries using enriched context, such as text, voice, and even visual data, offer significant advantages in customer banking and parametric insurance by enhancing user interaction, speed and overall service efficiency. Customers now have an option to choose their preferred mode of communication, whether through typing, speaking, or even using gestures. By analysing customer data from various sources, chatbots can offer personalised financial advice, investment recommendations, and alert about unusual activities. They even can help with the immediate payouts, by promptly verifying predefined parameters, such as weather data for crop insurance for example. However, with enriched context also come multi-dimensional ethical considerations, such as bias, fairness, transparency and confabulations. In this presentation I will cover how these risks emerge and mutually diffuse in highly automated interfaces.</td></td><td></td></tr><tr><td>14:40 - 15:20</td><td>    Community talks</td></tr><tr><td>15:20 - 15:30</td><td>    Break</td></tr><tr><td>15:30 - 16:10</td><td>    Keynote 4: Adam Steventon, Our Future Health</td></tr><tr><td><td style=word-wrap:break-word;max-width:810px;text-align:justify;padding-left:1.3em;text-indent:-1.3em>    <strong>Title:</strong> An incredibly detailed picture of human health: the exciting potential of Our Future Health to prevent, detect and treat diseases<br><strong>Abstract:</strong> In this presentation, I will detail the groundbreaking efforts of Our Future Health to construct a multimodal dataset encompassing 5 million individuals, representative of the UK&rsquo;s diverse population. I will explore the transformative potential of this dataset to enhance our capabilities in predicting, detecting, and treating major diseases. Additionally, I will discuss the roles of artificial intelligence in this context, focusing on the opportunities and challenges it presents. This exploration will underscore the potential of AI and large-scale data in shaping the future of healthcare.</td></td><td></td></tr><tr><td>16:10 - 16:40</td><td>    Panel discussion</td></tr><tr><td><td style=word-wrap:break-word;max-width:810px;text-align:justify><ul><li>What are the major barriers to deploying multimodal AI systems in real-world applications?</li><li>How can we best identify and utilise diverse data sources to advance multimodal AI research and applications?</li></td></td><td></td></tr><tr><td>16:40 - 17:00</td><td>    Best talk/poster prize winner announcement and closing</td></tr><tr><td>17:00 - 17:30</td><td>    Tea/coffee and networking</td></tr></tbody></table></center><center><p style=font-size:24px;font-weight:700>Morning Talk Session (10:40 - 11:20)</p><table><thead><tr><th>Name</th><th>    Title</th></tr></thead><tbody><tr><td>Yao Zhang</td><td>    AI in Maritime Engineering Control System</td></tr><tr><td>Douglas Amoke</td><td>    Geo-located Multimodal Data for Maritime Downstream Tasks</td></tr><tr><td>Yan Ge</td><td>    Multimodal Multi-task Asset Pricing with Numeral Learning</td></tr><tr><td>Ruyi Wang</td><td>    Multimodal Affective Computing for Mental Health Support</td></tr><tr><td>Luigi Moretti</td><td>    Integrating Affective Computing and Smart Sensing into Treatment Pathways for Anxiety Disorders</td></tr><tr><td>Jiawei Zheng</td><td>    Process-aware Human Activity Recognition</td></tr><tr><td>Lucas Farndale</td><td>    Super Vision without Supervision: Self-supervised Learning from Multimodal Data for Enhanced Biomedical Imaging</td></tr><tr><td>Ruizhe Li</td><td>    It&rsquo;s Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition</td></tr></tbody></table></center><center><p style=font-size:24px;font-weight:700>Afternoon Talk Session (14:40 - 15:20)</p><table><thead><tr><th>Name</th><th>    Title</th></tr></thead><tbody><tr><td>Valentin Danchev</td><td>    Data Governance, Ethics, and Safety of Multimodal vs Unimodal AI Models: A Review of Evidence and Challenges</td></tr><tr><td>Salah (Sam) Hammouche</td><td>    Beyond Regulatory Compliance: The RCR College Report</td></tr><tr><td>Nee Ling Wong</td><td>    How Would AI Work with Us in Healthcare</td></tr><tr><td>JZ</td><td>    Interdisciplinary Multimodal AI Research</td></tr><tr><td>Martin Callaghan</td><td>    Multimodal AI for Enhanced Information Extraction from Complex HPC Documentation</td></tr><tr><td>Madhurananda Pahar</td><td>    CognoSpeak: An Automatic, Remote Assessment of Early Cognitive Decline in Real-world Conversational Speech</td></tr><tr><td>Hubin Zhao</td><td>    Wearable Intelligent Multimodal Neuroimaging for Health</td></tr><tr><td>Peter Charlton</td><td>    Understanding Determinants of Health: Leveraging Routinely Collected Data</td></tr></tbody></table></center><center><p style=font-size:24px;font-weight:700>Posters</p><table><thead><tr><th>Name</th><th>    Title</th></tr></thead><tbody><tr><td>Douglas Amoke</td><td>    Geo-located Multimodal Data for Maritime Downstream Tasks</td></tr><tr><td>Sedat Dogan</td><td>    Enhanced Multimodal Learning for Meme Virality Prediction</td></tr><tr><td>Wenrui Fan</td><td>    MeDSLIP: Medical Dual-Stream Language-Image Pre-training for Fine-grained Alignment</td></tr><tr><td>Lucas Farndale</td><td>    Super Vision without Supervision: Self-Supervised Learning from Multimodal Data for Enhanced Biomedical Imaging</td></tr><tr><td>Yan Ge</td><td>    Multimodal Multi-Task Asset Pricing with Numeral Learning</td></tr><tr><td>Ruizhe Li</td><td>    Large Language Models are Efficient Learners of Noise-robust Speech Recognition</td></tr><tr><td>Xianyuan Liu</td><td>    <a href=../media/Poster_Exploring_Multimodal_AI_beyond_Vision_and_Language.pdf>Exploring Multimodal AI beyond Vision and Language</a></td></tr><tr><td>Sabrina McCallum</td><td>    Learning Generalisable Representations for Embodied Tasks with Multimodal Feedback</td></tr><tr><td>Luigi Moretti</td><td>    Integrating Affective Computing and Smart Sensing into Treatment Pathways for Anxiety Disorders</td></tr><tr><td>Madhuranand Pahar</td><td>    CognoSpeak: An Automatic, Remote Assessment of Early Cognitive Decline in Real-world Conversational Speech</td></tr><tr><td>Mohammod Suvon</td><td>    Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection</td></tr><tr><td>Ruyi Wang</td><td>    Multimodal Affects Spreading and Development</td></tr><tr><td>Jiawei Zheng</td><td>    Process-aware Human Activity Recognition</td></tr></tbody></table></center></div></div></div></section><section id=organiser class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Organising Committee</h1></div><div class="col-12 col-sm-auto people-person"><a href=/author/haiping-lu/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/haiping-lu/avatar_hucb84e28ca1e19b7e1577e26fab350297_1984751_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/haiping-lu/>Haiping Lu</a></h2><h3>Head of AI Research Engineering, Professor of Machine Learning, and Turing Academic Lead</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/david-clifton/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/david-clifton/avatar_hua594591fd8bde5abc5d3b81e0a80090c_59790_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/david-clifton/>David Clifton</a></h2><h3>Professor of Clinical Machine Learning, Department of Engineering Science, University of Oxford & Turing Fellow, Alan Turing Institute</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/shuo-zhou/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/shuo-zhou/avatar_hue0e91a9bb6916a97e67753bfe887cd9b_1646389_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/shuo-zhou/>Shuo Zhou</a></h2><h3>Deputy Head of AI Research Engineering, and Academic Fellow in Machine Learning</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/tingting-zhu/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/tingting-zhu/avatar_hu8296ee6266d0afef4d38a9b10fbec9ad_8409_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/tingting-zhu/>Tingting Zhu</a></h2><h3>Associate Professor in AI for Digital Health, University of Oxford</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/donghwan-shin/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/donghwan-shin/avatar_hu03f18d41ea59c605af85bffc612b3ac5_36286_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/donghwan-shin/>Donghwan Shin</a></h2><h3>Lecturer in Testing, School of Computer Science</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/peter-charlton/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/peter-charlton/avatar_hu9832f0fbf47147de4197ece20ae3646f_88560_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/peter-charlton/>Peter Charlton</a></h2><h3>British Heart Foundation Research Fellow, University of Cambridge</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/xianyuan-liu/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/xianyuan-liu/avatar_hu42e6bb4f77b4f8df2d26e0165d9cae14_23239_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/xianyuan-liu/>Xianyuan Liu</a></h2><h3>Assistant Head of AI Research Engineering, and Senior AI Research Engineer</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/chen-chen/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/chen-chen/avatar_hu1c16001ded50b9a6a830a8ad4aa3dccb_263158_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/chen-chen/>Chen Chen</a></h2><h3>Lecturer in Computer Vision, School of Computer Science</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/zhixiang-chen/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/zhixiang-chen/avatar_hu53d97f702b54cce4719f707188b70a95_45534_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/zhixiang-chen/>Zhixiang Chen</a></h2><h3>Lecturer in Machine Learning, School of Computer Science</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/emma-barker/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/emma-barker/avatar_hufefa501df927914a70e3327b8af7f544_70714_270x270_fill_lanczos_center_3.png alt=Avatar></a><div class=portrait-title><h2><a href=/author/emma-barker/>Emma Barker</a></h2><h3>Centre Manager of Centre for Machine Intelligence</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/xi-wang/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/xi-wang/avatar_huc84e63f6dc77192f086a5c0ce1be1001_92009_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/xi-wang/>Xi Wang</a></h2><h3>Lecturer in Natural Language Processing, School of Computer Science</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/sina-tabakhi/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/sina-tabakhi/avatar_hu2d09f6770ebfe31111cf5b827920c602_48516_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/sina-tabakhi/>Sina Tabakhi</a></h2><h3>PhD Student, School of Computer Science</h3></div></div></div></div></section><section id=technical class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Administrative and Technical Support</h1></div><div class="col-12 col-sm-auto people-person"><a href=/author/kate-jones/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/kate-jones/avatar_hu25ca4f581431378f68821530a5bc486a_4322132_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/kate-jones/>Kate Jones</a></h2><h3>Administrative Assistant for the Centre for Machine Intelligence</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/mohammod-suvon/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/mohammod-suvon/avatar_huf7be7f6950729f5661dac3da9684d920_115281_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/mohammod-suvon/>Mohammod Suvon</a></h2><h3>AI Research Engineer</h3></div></div></div></div></section><section id=partners class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Partners</h1></div><div class=col-12><p>This workshop is brought to you by the Turing Interest Group on Meta-Learning for Multimodal Data (welcome to <a href="https://forms.office.com/Pages/ResponsePage.aspx?id=p_SVQ1XklU-Knx-672OE-fR6PcyyBV1JuragBENwKPJUOFhHNkY5WU1RVlczMjNWUVdYTDFDME1VNSQlQCN0PWcu" target=_blank rel=noopener>sign up</a>) and the Multimodal AI Community (welcome to subscribe to our <a href="https://groups.google.com/a/sheffield.ac.uk/g/multimodal-ai-community-group?pli=1" target=_blank rel=noopener>Google Group</a>) supported by the <a href=https://www.sheffield.ac.uk/machine-intelligence target=_blank rel=noopener>Centre for Machine Intelligence</a> at the University of Sheffield.</p><div style=text-align:center><img src=/media/cmi_logo.png alt="CMI Logo" style=width:420px;height:auto;display:inline-block;margin-right:20px>
<img src=/media/ati_logo.jpg alt="ATI Logo" style=width:250px;height:auto;display:inline-block;margin-right:20px></div><p><strong>Disclaimer:</strong> This event is supported by The Alan Turing Institute. The Turing is not involved in the agenda or content planning.</p></div></div></div></section><section id=sponsors class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Our key sponsors</h1></div><div class=col-12><div style=text-align:center><a href=https://www.royce.ac.uk/ target=_blank><img src=/media/henry_royce_institute_logo.png alt="CMI Logo" style=width:420px;height:auto;display:inline-block;margin-right:20px>
</a><a href=https://learntechnique.com/ target=_blank><img src=/media/tls_logo.png alt="TLS Logo" style=width:420px;height:auto;display:inline-block;margin-right:20px></a></div></div></div></div></section><section id=contact class="home-section wg-contact"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Contact Us</h1></div><div class=col-12><img src=/media/the_edge_1.jpg style=width:620px;height:auto;display:inline-block>
<img src=/media/the_edge_2.webp style=width:620px;height:auto;display:inline-block><center><img src=/media/the_edge_3.jpg style=width:700px;height:auto></center><ul class=fa-ul><li><i class="fa-li fas fa-map-marker fa-2x" aria-hidden=true></i>
<span id=person-address>The Edge, The Endcliffe Village, 34 Endcliffe Cres, Sheffield, S10 3ED</span></li><li><i class="fa-li fas fa-envelope fa-2x" aria-hidden=true></i>
<a href=mailto:multimodal-ai-enquiry-group@shef.ac.uk>Email the organisers</a></li></ul><div class=d-none><input id=map-provider value=mapnik>
<input id=map-lat value=53.372795238367225>
<input id=map-lng value=-1.5074086064770071>
<input id=map-dir value="The Edge, The Endcliffe Village, 34 Endcliffe Cres, Sheffield, S10 3ED">
<input id=map-zoom value=15>
<input id=map-api-key value></div><div id=map></div></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 Multimodal AI Community UK.</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.6ab16275cbca742a586c1726e3d94093.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script></body></html>