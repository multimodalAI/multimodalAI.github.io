<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.b9231f75dcc97a371ce6141b4d2aadaf.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Haiping Lu"><meta name=description content><link rel=alternate hreflang=en-us href=https://multimodalAI.github.io/home/><link rel=canonical href=https://multimodalAI.github.io/home/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu885f0cfeed998fbf7dc869c80f754bbd_18100_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu885f0cfeed998fbf7dc869c80f754bbd_18100_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@MultimodalAI_UK"><meta property="twitter:creator" content="@MultimodalAI_UK"><meta property="twitter:image" content="https://multimodalAI.github.io/media/icon_hu885f0cfeed998fbf7dc869c80f754bbd_18100_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="MultimodalAIâ€™23"><meta property="og:url" content="https://multimodalAI.github.io/home/"><meta property="og:title" content="MultimodalAIâ€™23"><meta property="og:description" content><meta property="og:image" content="https://multimodalAI.github.io/media/icon_hu885f0cfeed998fbf7dc869c80f754bbd_18100_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><title>MultimodalAIâ€™23</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=8e7bc052bdfc6746ea2bb6595e8093eb><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>MultimodalAIâ€™23</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>MultimodalAIâ€™23</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>About</span></a></li><li class=nav-item><a class=nav-link href=/#speaker><span>Speakers</span></a></li><li class=nav-item><a class=nav-link href=/#programme><span>Programme</span></a></li><li class=nav-item><a class=nav-link href=/#organiser><span>Organisers</span></a></li><li class=nav-item><a class=nav-link href=/#partners><span>Partners</span></a></li><li class=nav-item><a class=nav-link href=https://onlineshop.shef.ac.uk/conferences-and-events/faculty-of-engineering/faculty-of-engineering/first-workshop-on-multimodal-ai target=_blank rel=noopener><span>Registration</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact Us</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=slider class="home-section wg-slider carousel slide" data-ride=carousel data-interval=false><div class=home-section-bg></div><ol class=carousel-indicators><li data-target=#slider data-slide-to=0 class=active></li><li data-target=#slider data-slide-to=1></li><li data-target=#slider data-slide-to=2></li></ol><div class=carousel-inner><div class="carousel-item active fullscreen" style=background-color:#666;background-image:url(https://multimodalAI.github.io/media/slide.jpg);background-repeat:no-repeat;background-position:100%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.6);backdrop-filter:brightness(.6)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:center><h1 class=hero-title>First Workshop on Multimodal AI</h1><p class=hero-lead style="margin:0 auto"><p><font size style="color: white !important;"><i>With unimodal AI maturing, what new doors will multimodal AI open for us?</i></font><br><font size=3 style="color: white !important;">In Person on 27th June 2023, Sheffield S10 3ED</font><div style=text-align:center><a href=https://twitter.com/MultimodalAI_UK class=btn style=margin:5px;background-color:#fff!important;color:purple!important><i class="fab fa-twitter"></i> Follow Us</a> <a href=/media/MultimodalAI23_Programme.pdf class=btn style=margin:5px;background-color:#fff!important;color:purple!important download><i class="fas fa-book"></i> Programme</a></p></div></p></div></div></div><div class="carousel-item fullscreen" style=background-image:url(https://multimodalAI.github.io/media/slider2.svg);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(1);backdrop-filter:brightness(1)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:left><h1 class=hero-title></h1></div></div></div><div class="carousel-item fullscreen" style=background-image:url(https://multimodalAI.github.io/media/slider3.png);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.7);backdrop-filter:brightness(.7)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:center><h1 class=hero-title>Opportunities</h1><p class=hero-lead style="margin:0 auto">Pitches and posters, four prizes - Â£150 each, and travel cost support</p></div></div></div></div><a class=carousel-control-prev href=#slider data-slide=prev><span class=carousel-control-prev-icon></span>
<span class=sr-only>Previous</span></a>
<a class=carousel-control-next href=#slider data-slide=next><span class=carousel-control-next-icon></span>
<span class=sr-only>Next</span></a></section><section id=speaker class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Keynote Speakers</h1></div><div class="col-12 col-sm-auto people-person"><a href=/author/yutian-chen/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/yutian-chen/avatar_huc7ce7117b1bc688bd01be078843ac25f_271169_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/yutian-chen/>Yutian Chen</a></h2><h3>Staff Research Scientist, Google DeepMind, AlphaGo Developer</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/mirella-lapata/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/mirella-lapata/avatar_hu8cb4dc84c4a20c76f39bf12e68d19b75_81189_270x270_fill_q75_lanczos_center.jpeg alt=Avatar></a><div class=portrait-title><h2><a href=/author/mirella-lapata/>Mirella Lapata</a></h2><h3>Professor of Natural Language Processing, University of Edinburgh & UKRI Turing AI World-Leading Researcher Fellow</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/chew-yean-yam/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/chew-yean-yam/avatar_hu02e7eb6a9173e23ae4b365a56ce5b844_64211_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/chew-yean-yam/>Chew-Yean Yam</a></h2><h3>Principal Data and Applied Scientist, Microsoft</h3></div></div></div></div></section><section id=about class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class=col-12><p style=color:purple;text-align:center;margin-bottom:0;font-weight:700>MultimodalAI'23 Photos Now Available! Click the photo to view and download!</p><a href="https://drive.google.com/drive/folders/1Mm3spa4EVAKJCJnoJpizEaP7MiP71o4Q?usp=sharing"><img src=media/group_photo.JPG alt="Group Photo" style=display:block;margin-left:auto;margin-right:auto;width:60%;margin-bottom:15px></a><p><b>Join the Multimodal AI Community mailing list by <a href=https://groups.google.com/a/sheffield.ac.uk/g/multimodal-ai-community-group target=_blank rel=noopener>subscribing to our Google Group</a>.</b></p><p style=text-align:justify>Multimodal AI combines multiple types of data (image, text, audio, etc) via machine learning models and algorithms to achieve better performance. Multimodal AI is key for AI research and applications including healthcare, net zero, finance, robotics, and manufacturing. Multimodal AI in these areas is challenging due to the inherent complexity of data integration and the limited availability of labelled data. Unimodal AI for a single type of data input is maturing at an accelerating pace, thus creating vast opportunities for tackling multimodal AI challenges.</p><p style=text-align:justify>MultimodalAIâ€™23 brings together researchers and practitioners from AI, data science, and various scientific and application domains to discuss problems and challenges, share experiences and solutions, explore collaborations and future directions, and build networks and a vibrant community on multimodal AI. We have three keynote speakers covering academic research, industrial research, and industrial applications: <a href=https://homepages.inf.ed.ac.uk/mlap/>Professor Mirella Lapata</a> (University of Edinburgh, UKRI Turing AI World-Leading Researcher Fellow), <a href=https://www.cantab.net/users/yutian.chen/index.html>Dr Yutian Chen</a> (Google DeepMind, AlphaGo Developer), and <a href="https://www.linkedin.com/in/cyyam/?originalSubdomain=uk">Dr Chew-Yean Yam</a> (Microsoft, Principal Data and Applied Scientist).</p><p style=text-align:justify>We offer participants opportunities to give 3-min pitches and present posters, with four prizes (Â£150 each) in total for the best pitches and best posters. You may submit proposals for a pitch and/or poster when you register. We will confirm accepted pitches and posters in the week ending June 17th.</p><p style=text-align:justify>Should you require assistance with accessibility for this event, or if you have any other special requirements, or if you would like to discuss your needs with the organizing team, please <a href=#contact>contact us</a>. We will do our best to fulfill your requirements to allow you to fully participate in this event.</p><p style=text-align:justify>Join this interdisciplinary event to create a diverse community that shapes and builds the future of multimodal AI research and developments.</p><p><strong>Welcome to share the workshop flyer in <a href=media/flyer.pdf>PDF</a> and <a href=media/flyer.png>PNG</a> with your network.</strong></p></div></div></div></section><section id=programme class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Final Programme (In Person Only)</h1><p>Tuesday, 27th June 2023</p></div><div class=col-md-12><center><table><thead><tr><th>Time</th><th>Â Â Â Â Event</th></tr></thead><tbody><tr><td>09:30 - 10:00</td><td>Â Â Â Â Registration and Morning Refreshments â˜•</td></tr><tr><td>10:00 - 10:10</td><td>Â Â Â Â Welcome and Introduction: Haiping Lu, The University of Sheffield ğŸ¤ (<a href="https://www.youtube.com/watch?v=MAhcVNaGNn0" target=_blank rel=noopener>YouTube Video</a>)</td></tr><tr><td>10:10 - 10:50</td><td>Â Â Â Â Keynote 1: Yutian Chen, Google DeepMind (<a href="https://www.youtube.com/watch?v=2_uicJgLbfs" target=_blank rel=noopener>YouTube Video</a>)</td></tr><tr><td><td style=word-wrap:break-word;max-width:300px;text-align:justify;padding-left:1.3em;text-indent:-1.3em>Â Â Â Â <strong>Title:</strong> Learning generalizable models on large scale multi-modal data<br><strong>Abstract:</strong> The abundant spectrum of multi-modal data provides a significant opportunity for augmenting the training of foundational models beyond mere text. In this talk, I will introduce two lines of work that leverage large-scale models, trained on Internet-scale multi-modal datasets, to achieve good generalization performance. The first work trains an audio-visual model on YouTube datasets of videos and enables automatic video translation and dubbing. The model is able to learn the correspondence between audio and visual features, and use this knowledge to translate videos from one language to another. The second work trains a multi-modal, multi-task, multi-embodiment generalist policy on a massive collection of simulated control tasks, vision, language, and robotics. The model is able to learn to perform a variety of tasks, including controlling a robot arm, playing a game, and translating text. Both lines of work exhibit the potential future trajectory of foundational models, highlighting the transformative power of integrating multi-modal inputs and outputs.</td></td><td></td></tr><tr><td>10:50 - 11:30</td><td>Â Â Â Â Morning 3-Minute Pitches ğŸ“£</td></tr><tr><td>11:30 - 12:00</td><td>Â Â Â Â Break and Poster Session 1 â˜• ğŸ–¼</td></tr><tr><td>12:00 - 12:40</td><td>Â Â Â Â Keynote 2: Mirella Lapata, The University of Edinburgh (<a href="https://www.youtube.com/watch?v=85uIICaLXPY" target=_blank rel=noopener>YouTube Video</a>)</td></tr><tr><td><td style=word-wrap:break-word;max-width:300px;text-align:justify;padding-left:1.3em;text-indent:-1.3em>Â Â Â Â <strong>Title:</strong> Hierarchical3D Adapters for Long Video-to-text Summarization<br><strong>Abstract:</strong> In this talk I will focus on video-to-text summarization and discuss how to best utilize multimodal information for summarizing long inputs (e.g., an hour-long TV show) into long outputs (e.g., a multi-sentence summary). We extend SummScreen (Chen et al., 2021), a dialogue summarization dataset consisting of transcripts of TV episodes with reference summaries, and create a multimodal variant by collecting corresponding fulllength videos. We incorporate multimodal information into a pretrained textual summarizer efficiently using adapter modules augmented with a hierarchical structure while tuning only 3.8% of model parameters. Our experiments demonstrate that multimodal information offers superior performance over more memory-heavy and fully fine-tuned textual summarization methods.</td></td><td></td></tr><tr><td>12:40 - 12:45</td><td>Â Â Â Â Group Photos</td></tr><tr><td>12:45 - 14:00</td><td>Â Â Â Â Lunch & Poster Session 2 ğŸ½ ğŸ–¼</td></tr><tr><td>14:00 - 14:40</td><td>Â Â Â Â Keynote 3: Chew-Yean Yam, Microsoft (<a href="https://www.youtube.com/watch?v=te1Rc_qd1V4" target=_blank rel=noopener>YouTube Video</a>)</td></tr><tr><td><td style=word-wrap:break-word;max-width:300px;text-align:justify;padding-left:1.3em;text-indent:-1.3em>Â Â Â Â <strong>Title:</strong> Us and AI: Redefining our relationships with AI<br><strong>Abstract:</strong> The rapid advancement of AI has transformed how we interact with intelligent machines. Unravel the dynamic shifts in human-AI relations across diverse roles that we play in our society. Spark your imagination and seize the power to sculpt this new relationship that is meaningful to you.</td></td><td></td></tr><tr><td>14:40 - 15:20</td><td>Â Â Â Â Afternoon 3-Minute Pitches ğŸ“£</td></tr><tr><td>15:20 - 15:25</td><td>Â Â Â Â Final Voting for Best Pitch/Poster and Best Student Pitch/Poster Prizes</td></tr><tr><td>15:25 - 16:00</td><td>Â Â Â Â Panel Discussion ğŸ’¬</td></tr><tr><td><td style=word-wrap:break-word;max-width:300px;text-align:justify><ul><li>What breakthroughs in multimodal AI do you foresee having the most significant impact in the next five years?</li><li>How can we navigate and mitigate the ethical concerns associated with advancing multimodal AI technologies?</li><li>Given the interdisciplinary nature of multimodal AI, how can we better integrate different fields of expertise to accelerate innovation in this area?</li></td></td><td></td></tr><tr><td>16:00 - 16:10</td><td>Â Â Â Â Prize Winner Announcement and Closing Initiative: Haiping Lu ğŸ†</td></tr><tr><td>16:10 - 17:00</td><td>Â Â Â Â Tea/Coffee and Networking Forums ğŸµ â˜•</td></tr><tr><td></td><td><ul><li>Envisioning MultimodalAI'24</li><li>Boosting Engagement and Active Participation in Multimodal AI</li><li>Cross-Disciplinary Collaboration and Resource Sharing in Multimodal AI</li><li>Open-Source Software Development for Multimodal AI</li><li>Ethical and Responsible Practices in Multimodal AI</li></ul></td></tr></tbody></table></center><center><p style=font-size:24px;font-weight:700>Morning Pitch Session</p><table><thead><tr><th>Name</th><th>Â Â Â Â Title</th></tr></thead><tbody><tr><td>Xingchi Liu<td style=word-wrap:break-word;max-width:300px;text-align:justify;padding-left:1.3em;text-indent:-1.3em>Â Â Â Â A Gaussian Process Method for Ground Vehicle Classification using Acoustic Data</td></td><td></td></tr><tr><td>Adam Wynn</td><td>Â Â Â Â BETTER: An automatic feedBack systEm for supporTing emoTional spEech tRaining (<a href="https://drive.google.com/file/d/1S2bDpopkfiaagFOwpgGazc64njZudHNa/view?usp=drive_link" target=_blank rel=noopener>slides</a>)</td></tr><tr><td>Yizhi Li</td><td>Â Â Â Â MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training</td></tr><tr><td>Rui Zhu</td><td>Â Â Â Â KnowWhereGraph: A Geospatial Knowledge Graph to Support Cross-Domain Knowledge Discovery</td></tr><tr><td>Imene Tarakli</td><td>Â Â Â Â Robot as a Schoolmates for Enhanced Adaptive Learning</td></tr><tr><td>Sina Tabakhi</td><td>Â Â Â Â From Multimodal Learning to Graph Neural Networks (<a href="https://drive.google.com/file/d/1BNJPjNwxma_0D-e_6CvnqxgJoeQusB-M/view?usp=drive_link" target=_blank rel=noopener>slides</a>)</td></tr><tr><td>Nitisha Jain</td><td>Â Â Â Â Semantic Interpretations of Multimodal Embeddings towards Explainable AI</td></tr></tbody></table></center><center><p style=font-size:24px;font-weight:700>Afternoon Pitch Session</p><table><thead><tr><th>Name</th><th>Â Â Â Â Title</th></tr></thead><tbody><tr><td>Lucia Cipolina-Kun<td style=word-wrap:break-word;max-width:300px;text-align:justify;padding-left:1.3em;text-indent:-1.3em>Â Â Â Â Diffusion Models for the Restoration of Cultural Heritage (<a href="https://drive.google.com/file/d/1HDfhWicOKIxALlVOw9USgE-W0WSStUcc/view?usp=drive_link" target=_blank rel=noopener>slides</a>)</td></td><td></td></tr><tr><td>Bohua Peng</td><td>Â Â Â Â Recent Findings of Foundational Models on Multimodal NLU</td></tr><tr><td>Thao Do</td><td>Â Â Â Â Social Media Mining and Machine Learning for Understanding Illegal Wildlife Trade in Vietnam (<a href="https://drive.google.com/file/d/1bd_S3V2ynXzBn_CL3tJYFVX16ZSRmEV6/view?usp=sharing" target=_blank rel=noopener>slides</a>)</td></tr><tr><td>Sam Barnett</td><td>Â Â Â Â The Genomics England Multimodal Research Environment for Medical Research</td></tr><tr><td>Luigi A. Moretti<td style=word-wrap:break-word;max-width:250px;text-align:justify;padding-left:1em;text-indent:-.9em>Â  Â Â Can AI be effectively implemented to help treat Anxiety Disorders (ADs)? (<a href="https://drive.google.com/file/d/1xaW8lRwstYN5MCN6k95-J4HiPxnKGCtF/view?usp=sharing" target=_blank rel=noopener>slides</a>)</td></td><td></td></tr><tr><td>Mohammod Suvon</td><td>Â Â Â Â The Multimodal Lens: Understanding of Radiologists Visual Search Behavior Patterns (<a href="https://drive.google.com/file/d/1A4VDgEvzuEZ2aL_R6_IiMclwkTEDFwuu/view?usp=sharing" target=_blank rel=noopener>slides</a>)</td></tr><tr><td>Ning Ma</td><td>Â Â Â Â Obstructive Sleep Apnoea Screening with Breathing Sounds and Respiratory Effort: A Multimodal Deep Learning Approach (<a href="https://drive.google.com/file/d/1-OnnkKMBMj-M4FSn0pIOHPlGVLVCsSLM/view?usp=drive_link" target=_blank rel=noopener>slides</a>)</td></tr><tr><td>Felix Krones</td><td>Â Â Â Â Multimodal Cardiomegaly Classification with Image-Derived Digital Biomarkers (<a href="https://drive.google.com/file/d/14ZOzsLPLMtiguXXhG-GSHMd45wBe2Qfq/view?usp=sharing" target=_blank rel=noopener>sildes</a>)</td></tr></tbody></table></center><center><p style=font-size:24px;font-weight:700>Posters</p><table><thead><tr><th>Name</th><th>Â Â Â Â Title</th></tr></thead><tbody><tr><td>Abdulsalam Alsunaidi</td><td>Â Â Â Â Predicting Actions in Images using Distributed Lexical Representations</td></tr><tr><td>Bohua Peng</td><td>Â Â Â Â Recent Findings of Foundational Models on Multimodal NLU</td></tr><tr><td>Chenghao Xiao</td><td>Â Â Â Â Adversarial Length Attack to Vision-Language Models</td></tr><tr><td>Chenyang Wang</td><td>Â Â Â Â A Novel Multimodal AI Model for Generating Hospital Discharge Instruction</td></tr><tr><td>Christoforos Galazis</td><td>Â Â Â Â High-resolution 3D Maps of Left Atrial Displacements using an Unsupervised Image Registration Neural Network</td></tr><tr><td>Douglas Amoke</td><td>Â Â Â Â Multimodal Data and AI for Downstream Tasks</td></tr><tr><td>Jayani Bhatwadiya</td><td>Â Â Â Â Multimodal AI for Cancer Detection and Diagnosis : A Study on the Cancer Imaging Archive (TCIA) Dataset</td></tr><tr><td>Jiachen Luo</td><td>Â Â Â Â Cross-Modal Fusion Techniques for Emotion Recognition from Text and Speech</td></tr><tr><td>Jingkun Chen</td><td>Â Â Â Â Semi-Supervised Unpaired Medical Image Segmentation Through Task-Affinity Consistency</td></tr><tr><td>Lucia Cipolina-Kun</td><td>Â Â Â Â Diffusion Models for the Restoration of Cultural Heritage (<a href="https://drive.google.com/file/d/1lnOLLMXExT0V7CKJ-hRpshFbF5h3TdLS/view?usp=drive_link" target=_blank rel=noopener>poster</a>)</td></tr><tr><td>Luigi A. Moretti<td style=word-wrap:break-word;max-width:250px;text-align:justify;padding-left:1.1em;text-indent:-1em>Â Â Â Â Can AI be effectively implemented to help treat Anxiety Disorders (ADs)? (<a href="https://drive.google.com/file/d/1JTnJEGUoG7yKXPROXU59amOhkv5kQPwJ/view?usp=drive_link" target=_blank rel=noopener>poster</a>)</td></td><td></td></tr><tr><td>Nitisha Jain</td><td>Â Â Â Â Semantic Interpretations of Multimodal Embeddings towards Explainable AI</td></tr><tr><td>Prasun Tripathi</td><td>Â Â Â Â Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI</td></tr><tr><td>Raja Omman Zafar</td><td>Â Â Â Â Digital Twin for Homecare</td></tr><tr><td>Sokratia Georgaka</td><td>Â Â Â Â CellPie: A Fast Spatial Transcriptomics Topic Discovery Method via Joint Factorization of Gene Expression and Imaging Data</td></tr><tr><td>Wei Xing</td><td>Â Â Â Â Multi-Fidelity Fusion (<a href="https://drive.google.com/file/d/1XPHgNrTfB6F6dCxRNaI88QXvjKU4OMyy/view?usp=drive_link" target=_blank rel=noopener>poster</a>)</td></tr><tr><td>Yichen He</td><td>Â Â Â Â AI in Evolution and Ecology (<a href="https://drive.google.com/file/d/1DdCfqnaY1WHKiGW9mT22gclUMaDtgJlq/view?usp=drive_link" target=_blank rel=noopener>poster</a>)</td></tr><tr><td>Yixuan Zhu</td><td>Â Â Â Â Potential Multimodal AI for Electroencephalogram (EEG) Analysis (<a href="https://drive.google.com/file/d/17AMDRQPg7ag4TpAp1cLv6YWD50A4JmnO/view?usp=drive_link" target=_blank rel=noopener>poster</a>)</td></tr><tr><td>Yizhi Li</td><td>Â Â Â Â MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training (<a href="https://drive.google.com/file/d/1F4Syz-Ab8tZR-qXnsXXpm3F1HWah18k2/view?usp=drive_link" target=_blank rel=noopener>poster</a>)</td></tr><tr><td>Yu Hon On</td><td>Â Â Â Â Automatic Aortic Valve Disease Detection from MRI with Spatio-Temporal Attention Maps</td></tr></tbody></table></center></div></div></div></section><section id=organiser class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Organising Committee</h1></div><div class="col-12 col-sm-auto people-person"><a href=/author/haiping-lu/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/haiping-lu/avatar_huaea986fe85a28b20cb3e8d66de66f869_1846195_270x270_fill_lanczos_center_3.png alt=Avatar></a><div class=portrait-title><h2><a href=/author/haiping-lu/>Haiping Lu</a></h2><h3>Professor of Machine Learning, and Head of AI Research Engineering, University of Sheffield & Turing Academic Lead, Alan Turing Institute</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/david-clifton/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/david-clifton/avatar_hua594591fd8bde5abc5d3b81e0a80090c_59790_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/david-clifton/>David Clifton</a></h2><h3>Professor of Clinical Machine Learning, Department of Engineering Science, University of Oxford & Turing Fellow, Alan Turing Institute</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/tingting-zhu/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/tingting-zhu/avatar_hu8296ee6266d0afef4d38a9b10fbec9ad_8409_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/tingting-zhu/>Tingting Zhu</a></h2><h3>Associate Professor in AI for Digital Health, University of Oxford</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/peter-charlton/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/peter-charlton/avatar_hu9832f0fbf47147de4197ece20ae3646f_88560_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/peter-charlton/>Peter Charlton</a></h2><h3>British Heart Foundation Research Fellow, University of Cambridge</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/tingyan-tina-wang/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/tingyan-tina-wang/avatar_hubd283770503298ba67b52d7bc1c51e62_397363_270x270_fill_lanczos_center_3.png alt=Avatar></a><div class=portrait-title><h2><a href=/author/tingyan-tina-wang/>Tingyan (Tina) Wang</a></h2><h3>Postdoctoral Scientist, University of Oxford</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/shuo-zhou/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/shuo-zhou/avatar_hue0e91a9bb6916a97e67753bfe887cd9b_1646389_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/shuo-zhou/>Shuo Zhou</a></h2><h3>Academic Fellow, Department of Computer Science, University of Sheffield</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/lei-lu/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/lei-lu/avatar_hue773deeae6be606539a98e850068fb3f_9917_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/lei-lu/>Lei Lu</a></h2><h3>Postdoctoral Research Assistant, University of Oxford</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/zhixiang-chen/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/zhixiang-chen/avatar_hu53d97f702b54cce4719f707188b70a95_45534_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/zhixiang-chen/>Zhixiang Chen</a></h2><h3>Lecturer in Machine Learning, Department of Computer Science, The University of Sheffield</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/emma-barker/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/emma-barker/avatar_hufefa501df927914a70e3327b8af7f544_70714_270x270_fill_lanczos_center_3.png alt=Avatar></a><div class=portrait-title><h2><a href=/author/emma-barker/>Emma Barker</a></h2><h3>Research Fellow and Data Science & AI Community Manager, University of Sheffield</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/sina-tabakhi/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/sina-tabakhi/avatar_hu2d09f6770ebfe31111cf5b827920c602_48516_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/sina-tabakhi/>Sina Tabakhi</a></h2><h3>PhD Student, Department of Computer Science, University of Sheffield</h3></div></div></div></div></section><section id=technical class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Technical Support</h1></div><div class="col-12 col-sm-auto people-person"><a href=/author/mohammod-naimul-islam-suvon/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/mohammod-naimul-islam-suvon/avatar_hua7c6d7af523f184109321e1e21c61ca9_170524_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/mohammod-naimul-islam-suvon/>Mohammod Naimul Islam Suvon</a></h2><h3>Research Assistant, Department of Computer Science, University of Sheffield</h3></div></div><div class="col-12 col-sm-auto people-person"><a href=/author/jayani-bhatwadiya/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/jayani-bhatwadiya/avatar_huae62ae838ba329430bdf9af23267ecd2_245394_270x270_fill_lanczos_center_3.png alt=Avatar></a><div class=portrait-title><h2><a href=/author/jayani-bhatwadiya/>Jayani Bhatwadiya</a></h2><h3>Data Analyst, Nuffield Department of Population Health, University of Oxford</h3></div></div></div></div></section><section id=partners class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Sponsors and Partners</h1></div><div class=col-12><p style=text-align:justify>This workshop is jointly organised by University of Sheffield and University of Oxford under the Turing Network Funding from the Alan Turing Institute, with support from University of Sheffieldâ€™s <a href=https://www.sheffield.ac.uk/machine-intelligence>Centre for Machine Intelligence</a> and Alan Turing Instituteâ€™s Interest Group on <a href=https://www.turing.ac.uk/research/interest-groups/meta-learning-multimodal-data>Meta-learning for Multimodal Data</a> (welcome to <a href="https://forms.office.com/Pages/ResponsePage.aspx?id=p_SVQ1XklU-Knx-672OE-fR6PcyyBV1JuragBENwKPJUOFhHNkY5WU1RVlczMjNWUVdYTDFDME1VNSQlQCN0PWcu">sign-up and join</a>).</p><div style=text-align:center><img src=/media/cmi_logo.png alt="CMI Logo" style=width:420px;height:auto;display:inline-block;margin-right:20px>
<img src=/media/ati_logo.jpg alt="ATI Logo" style=width:250px;height:auto;display:inline-block;margin-right:20px>
<img src=/media/ox_logo.png alt="OX Logo" style=width:200px;height:auto;display:inline-block></div><br><p style=text-align:justify><b>Disclaimer:</b> This event is supported by The Alan Turing Institute. The Turing is not involved in the agenda or content planning.</p></div></div></div></section><section id=contact class="home-section wg-contact"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Contact Us</h1></div><div class=col-12><img src=/media/the_edge_1.jpg style=width:620px;height:auto;display:inline-block>
<img src=/media/the_edge_2.webp style=width:620px;height:auto;display:inline-block>
<center><img src=/media/the_edge_3.jpg style=width:700px;height:auto></center><ul class=fa-ul><li><i class="fa-li fas fa-map-marker fa-2x" aria-hidden=true></i>
<span id=person-address>The Edge, The Endcliffe Village, 34 Endcliffe Cres, Sheffield, S10 3ED</span></li><li><i class="fa-li fas fa-envelope fa-2x" aria-hidden=true></i>
<a href=mailto:multimodal-ai-enquiry-group@shef.ac.uk>Email the organisers</a></li></ul><div class=d-none><input id=map-provider value=mapnik>
<input id=map-lat value=53.372795238367225>
<input id=map-lng value=-1.5074086064770071>
<input id=map-dir value="The Edge, The Endcliffe Village, 34 Endcliffe Cres, Sheffield, S10 3ED">
<input id=map-zoom value=15>
<input id=map-api-key value></div><div id=map></div></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">Â© 2023 Multimodal AI Workshop.</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> â€” the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.32ee83730ed883becad04bc5170512cc.js></script>
<script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.91534f6cb18c3621254d412c69186d7c.js></script>
<script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script></body></html>