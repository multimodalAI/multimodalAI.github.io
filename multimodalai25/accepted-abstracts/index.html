<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.b9231f75dcc97a371ce6141b4d2aadaf.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><script async src="https://www.googletagmanager.com/gtag/js?id=G-5K2K2GHWHQ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","G-5K2K2GHWHQ",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><meta name=author content="Haiping Lu"><meta name=description content><link rel=alternate hreflang=en-us href=https://multimodalAI.github.io/multimodalai25/accepted-abstracts/><link rel=canonical href=https://multimodalAI.github.io/multimodalai25/accepted-abstracts/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu25caa61f2e6d310cd2b668efa4781043_104298_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu25caa61f2e6d310cd2b668efa4781043_104298_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@MultimodalAI_UK"><meta property="twitter:creator" content="@MultimodalAI_UK"><meta property="twitter:image" content="https://multimodalAI.github.io/media/icon_hu25caa61f2e6d310cd2b668efa4781043_104298_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="UK Open Multimodal AI Network"><meta property="og:url" content="https://multimodalAI.github.io/multimodalai25/accepted-abstracts/"><meta property="og:title" content="New | UK Open Multimodal AI Network"><meta property="og:description" content><meta property="og:image" content="https://multimodalAI.github.io/media/icon_hu25caa61f2e6d310cd2b668efa4781043_104298_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><title>New | UK Open Multimodal AI Network</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=4394369941b98e7060c5c7a3d21aa9e1><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>UK Open Multimodal AI Network</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>UK Open Multimodal AI Network</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/omaib><span>Funding Call</span></a></li><li class=nav-item><a class=nav-link href=/multimodalai25 data-target=[]><span>MultimodalAI'25</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>About</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/#ukomain><span>UKOMAIN</span></a>
<a class=dropdown-item href=/#team><span>Team</span></a>
<a class=dropdown-item href=/#contact><span>Contact</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Past Events</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/embc25workshop><span>EMBC'25Workshop</span></a>
<a class=dropdown-item href=/multimodalai24><span>MultimodalAI'24</span></a>
<a class=dropdown-item href=/multimodalai-forum24><span>MultimodalAIForum'24</span></a>
<a class=dropdown-item href=/multimodalai-sprint23><span>MultimodalAISprint'23</span></a>
<a class=dropdown-item href=/multimodalai23><span>MultimodalAI'23</span></a></div></li><li class=nav-item><a class=nav-link href=/code-of-conduct><span>Code of Conduct</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=abstract-list class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Accepted Abstracts</h1><p>Third Workshop on Multimodal AI · 16–17 September 2025 · London, UK</p></div><div class=col-md-12><style>.sticky-buttons{position:fixed;top:6px!important;left:50%;transform:translateX(-50%);background:rgba(255,255,255,.9);padding:5px 8px;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);z-index:9999;display:flex;flex-direction:row;flex-wrap:nowrap;overflow-x:auto;max-width:100vw}.sticky-buttons button{font-family:open sans,Arial,sans-serif;font-size:14px;font-weight:700;padding:4px 12px;border:none;border-radius:4px;background-color:#154c79;color:#abdbe3;cursor:pointer;margin-right:8px;flex:none;min-width:120px;white-space:nowrap}</style><center><table><thead><tr><th>Name</th><th>    Title</th></tr></thead><tbody><tr><td>Hazrat Ali</td><td>From Pixels to Procedures: Structured Surgical Scene Understanding via Multimodal Large Language Models</td></tr><tr><td>Sedat DOGAN</td><td>Early Prediction of Multimodal Cross-Lingual Meme Virality</td></tr><tr><td>Swapnil Bhosale</td><td>Holistic Scene Representations for Immersive Audio Synthesis</td></tr><tr><td>Hanwen Xing</td><td>Multiomics Data Integration via Neighbourhood Preservation</td></tr><tr><td>Zainab Almugbel</td><td>Multi-Modal MAML: Revisiting Features Fusion for Discriminative Generalization and Class Distribution</td></tr><tr><td>Enrico Parisini</td><td>Concept-Based Modelling for Multimodal Flows</td></tr><tr><td>Emmanouil Benetos</td><td>Multimodal Music Understanding</td></tr><tr><td>Qifan Fu</td><td>Gesture Space Quantized Mixture of Experts</td></tr><tr><td>Alessandro Suglia</td><td>Pixel-Based Language Models: A Unified Approach to Multimodal AI Agents</td></tr><tr><td>Lu Gan</td><td>A Lightweight Multimodal Audio Scene Classification Framework via Knowledge Distillation</td></tr><tr><td>Siyi Du</td><td>STiL: Semi-supervised Tabular-Image Learning for Comprehensive Task-Relevant Information Exploration in Multimodal Classification</td></tr><tr><td>Konstantin Georgiev</td><td>MM-HealthFair: A Novel Framework for Quantifying and Mitigating Healthcare Biases in Multimodal AI Algorithms for Risk Prediction</td></tr><tr><td>Sabrina McCallum</td><td>GPTNT: Evaluating Multimodal Language Model Agents in Time-Critical Collaborative Tasks</td></tr><tr><td>Stephan Goerttler</td><td>Stochastic Graph Heat Modelling for Cross-Modal Connectivity Estimation</td></tr><tr><td>Adam Wynn</td><td>Semi-supervised Speech Confidence Detection using Whisper Embeddings</td></tr><tr><td>Mohammad Aadil Minhaz</td><td>Multimodal AI Security: Mitigating Prompt Attacks on AI Models using AI-Gateway</td></tr><tr><td>Wenrui Fan</td><td>Foundation-Model-Boosted Multimodal Learning for fMRI-based Neuropathic Pain Drug Response Prediction</td></tr><tr><td>Hanya Tamer Ahmed</td><td>Bridging Species with AI: A Cross-Species Deep Learning Model for Fracture Detection and Beyond</td></tr><tr><td>Junxi Zhang</td><td>Decoding Ambiguity: A Multimodal Dataset for Ambiguous Actions in Manufacturing</td></tr><tr><td>Tadiyos Hailemichael Mamo</td><td>Causal Learning for Enhanced Chronic Disease Management and Interventions</td></tr><tr><td>Minoru Dhananjaya Jayakody Arachchige</td><td>Multimodal Inspection of End-of-Life Components</td></tr><tr><td>Xinxing Ren</td><td>SimuGen: Multi-Modal Agentic Framework for Constructing Block Diagram-Based Simulation Models</td></tr><tr><td>Munib Mesinovic</td><td>MM-GraphSurv: Interpretable Multi-Modal Graph for Survival Prediction with Electronic Health Records</td></tr><tr><td>Fiona Young</td><td>Crossmodal Contrastive Learning with Pathology and Transcriptomics</td></tr><tr><td>Angeline Wang</td><td>Neural Substrates of Affective Empathy: Interactions between ACC and InC</td></tr><tr><td>Fan Guo</td><td>Enhancing Negotiation Policies via Spatio-Temporal Directed Graphs for Autonomous Interaction</td></tr><tr><td>Misbah Rafique</td><td>Realistic Galaxy Images Through Generative Adversarial Network</td></tr><tr><td>Mohammod Suvon</td><td>Multimodal Latent Fusion of ECG Leads for Early Assessment of Pulmonary Hypertension</td></tr><tr><td>Carolina Scarton</td><td>AI-TRACE: AI-driven mulTimodal and tempoRal disinformAtion analysis models in Continuous data strEams</td></tr><tr><td>Mingcheng Zhu</td><td>From Byte Pair to Token Pair: Efficient Prompt Compression for Large Language Models in Clinical Prediction</td></tr><tr><td>Boyu Chen</td><td>Robust Multimodal Autonomous Driving Perception under Occlusions</td></tr><tr><td>Jingzhi Ruan</td><td>A Multi-Scale Tactile-Visual-Text Alignment Framework Driven by Large Models</td></tr><tr><td>Chenqi Li</td><td>Multi-Teacher Distillation for Multimodal Biosignal Foundation Models</td></tr><tr><td>Chenqi Li</td><td>BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals</td></tr><tr><td>David Western</td><td>Fusion of a Priori Clinical Text Enhances Abnormal EEG Classification</td></tr><tr><td>Lu Gan</td><td>Digital Twins and Multimodal AI for Net Zero Housing</td></tr><tr><td>Chen Chen</td><td>Advancing Cardiac Care through Multi-Modal Data Integration for Precise Scar Mapping</td></tr><tr><td>Farheen Ramzan</td><td>CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse Myocardial Scar Synthesis and Segmentation</td></tr><tr><td>Jessica Fan</td><td>Tiered Vibe Mapping (TVM): A Feature-Space Decomposition for Aesthetic Modeling</td></tr><tr><td>Shibo Li</td><td>A Unified Multi-modal Foundation Model for Medical Imaging Synthesis and Diagnosis</td></tr><tr><td>Hegel Pedroza</td><td>Guitar-TECHS: An Electric Guitar Dataset Covering Techniques, Musical Excerpts, Chords, and Scales</td></tr><tr><td>Wenjing Zhao</td><td>Multimodal Approach in Corner Case Generation for Autonomous Driving</td></tr><tr><td>Teng Gao</td><td>Non-causal Economic Model Predictive Control for Wave Energy Converter</td></tr><tr><td>Vincentius Versandy Wijaya</td><td>Non-causal Model Predictive Control for Wave Energy Converters Based on Physics-Informed Neural Networks</td></tr><tr><td>Kewei Zhu</td><td>ReadMOF: Structure-Free Semantic Embeddings from Systematic MOF Nomenclature for Machine Learning Applications</td></tr><tr><td>Gaoyun Fang</td><td>Understanding Multimodal Fusion through Cross-Modal Interaction</td></tr><tr><td>Ruby Wood</td><td>Multimodal AI for Prediction of Response to Immunotherapy in Cancer Patients</td></tr><tr><td>Rahul Singh Maharjan</td><td>FM-OVD: Towards Fast Open-Vocabulary Object Detection with Feature-wise Modulation</td></tr><tr><td>Harry Findlay</td><td>Multimodal Perception and Representation Learning in Human Behaviour Modelling</td></tr><tr><td>Harshith Yerraguntla</td><td>Multimodal Glucose Forecasting with Physics-Informed Neural Networks for Type 1 Diabetes</td></tr><tr><td>Jason Lo</td><td>From Data to Concepts via Wiring Diagrams</td></tr><tr><td>Zixuan Huang</td><td>Multimodal RL-Diffusion Framework for Automated Generation of High-Risk Scenarios in Autonomous Vehicle Safety Testing</td></tr><tr><td>Tianyi Jiang</td><td>Multi-Modal Representation Learning for Molecular Property Prediction: Sequence, Graph, Geometry</td></tr><tr><td>Sneha Roychowdhury</td><td>Towards a Standardised Framework for Explainable AI in Healthcare using Non-imaging Data: Integrating User-Centric Perspectives, Evaluation Metrics, and Contextual Expertise</td></tr><tr><td>Mingrui Ye</td><td>Can MLLMs be Students&rsquo; Art Mentor? A Multi-Dimensional Benchmark Towards Comprehensive Assessment and Pedagogical Feedback</td></tr><tr><td>Awais Rauf</td><td>Efficient 3D-Aware Facial Image Editing via Attribute-Specific Prompt Learning</td></tr><tr><td>Awais Rauf</td><td>Bridging Domain Gaps in Specialized Fields: Multimodal Foundation Models for Sustainable Agriculture</td></tr><tr><td>Nasim Mohamed Ismail</td><td>Addressing Systematic Bias in Multimodal Integration for Alzheimer’s Disease Classification</td></tr><tr><td>Noor Ul Ain Zahra</td><td>AI-Driven Validation: Predicting NMR Spectra to Assess the Fidelity of AlphaFold Structures</td></tr><tr><td>Yutong Song</td><td>Proactive Multi-Agent Reinforcement Learning for Search and Rescue in Stochastic Ocean Environmen</td></tr><tr><td>L. M. Riza Rizky</td><td>Interpretable Multimodal Machine Learning for Identifying Drug Treatment Response Biomarkers in Neuropathic Pain</td></tr><tr><td>Jiani Chen</td><td>Learning-Based Wave Prediction</td></tr><tr><td>Haolin Wang</td><td>Benchmarking Band Gap Prediction For Semiconductor Materials Using Multimodal And Multi-Fidelity Data</td></tr><tr><td>Li Zhang</td><td>Integrating Heterogeneous Data Sources to Enhance Trading Strategies in Commodity Futures Markets</td></tr><tr><td>Xianyuan Liu</td><td>Geometry-Aware Line Graph Transformer Pre-training for Molecular Property Prediction</td></tr><tr><td>Sina Tabakhi</td><td>Missing-Modality-Aware Graph Neural Network for Cancer Classification</td></tr><tr><td>Xianyuan Liu</td><td>Towards Deployment-Centric Multimodal AI Beyond Vision and Language</td></tr><tr><td>Jiin Woei Lee</td><td>Interpretable Multimodal AI for Predicting Early Biological Cell Responses to Biomaterial Implant Coatings</td></tr><tr><td>Zhongtian Sun</td><td>Hybrid Framework for Lifelong Medical Imaging</td></tr><tr><td>Ziming Liu</td><td>A City-Scale Multimodal Dataset and Benchmark Suite for AI-Driven Radio Resource Control in Wireless Networks</td></tr><tr><td>Luigi A. Moretti</td><td>A Multimodal Affective Computing Pipeline for Correlating Physiological and Subjective Data Streams in Anxiety Disorders Management</td></tr><tr><td>Valentin Danchev</td><td>Evaluation of Risks of Overreliance on AI Multimodal Models</td></tr><tr><td>Abdul Ghani Zahid</td><td>Physics-Guided Domain-Aware Deep Learning for Robust Wireless Modulation Classification</td></tr><tr><td>Gaoyun Fang</td><td>Understanding Multimodal Fusion through Cross-Modal Interaction</td></tr><tr><td>Halimat Afolabi</td><td>Examining Modality‑Dependent Explanations and Reasoning Shifts in Closed Multimodal LLMs for Emotion Recognition</td></tr><tr><td>Daniel Onah</td><td>Benchmarking Machine Learning Ensemble Algorithms for a Classification Task</td></tr></tbody></table></center></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 UK Open Multimodal AI Network (UKOMAIN).<br><a href=/privacy-and-data-use/>Privacy and Data Use Notice</a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.6ab16275cbca742a586c1726e3d94093.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script></body></html>