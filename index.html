<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.b9231f75dcc97a371ce6141b4d2aadaf.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Haiping Lu"><meta name=description content="Unleashing the Potential of Multimodal AI - Join Us at Our First Workshop!"><link rel=alternate hreflang=en-us href=https://multimodalAI.github.io/><link rel=canonical href=https://multimodalAI.github.io/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu48a82c8450dfe9aca8259bacb1bf2d41_406640_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu48a82c8450dfe9aca8259bacb1bf2d41_406640_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@MultimodalAI_UK"><meta property="twitter:creator" content="@MultimodalAI_UK"><meta property="twitter:image" content="https://multimodalAI.github.io/media/icon_hu48a82c8450dfe9aca8259bacb1bf2d41_406640_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Multimodal AI Community UK"><meta property="og:url" content="https://multimodalAI.github.io/"><meta property="og:title" content="Multimodal AI Community UK"><meta property="og:description" content="Unleashing the Potential of Multimodal AI - Join Us at Our First Workshop!"><meta property="og:image" content="https://multimodalAI.github.io/media/icon_hu48a82c8450dfe9aca8259bacb1bf2d41_406640_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://multimodalAI.github.io/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://multimodalAI.github.io/"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","@id":"https://multimodalAI.github.io/","name":"Unleashing the Potential of Multimodal AI - Join Us at Our First Workshop!","logo":"https://multimodalAI.github.io/media/icon_hu48a82c8450dfe9aca8259bacb1bf2d41_406640_192x192_fill_lanczos_center_3.png","url":"https://multimodalAI.github.io/"}</script><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script><link rel=alternate href=/index.xml type=application/rss+xml title="Multimodal AI Community UK"><title>Multimodal AI Community UK</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Multimodal AI Community UK</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Multimodal AI Community UK</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#><span>Current Event</span></a></li><li class=nav-item><a class=nav-link href=/#programme data-target=#programme><span>Programme</span></a></li><li class=nav-item><a class=nav-link href=/#contact data-target=#contact><span>Contact Us</span></a></li><li class=nav-item><a class=nav-link href=/#direction data-target=#direction><span>Direction</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Past Events</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/mlai data-target=mlai><span>MultimodalAI'23</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=slider class="home-section wg-slider carousel slide" data-ride=carousel data-interval=false><div class=home-section-bg></div><ol class=carousel-indicators><li data-target=#slider data-slide-to=0 class=active></li></ol><div class=carousel-inner><div class="carousel-item active fullscreen" style=background-color:#666;background-image:url(https://multimodalAI.github.io/media/slide.jpg);background-repeat:no-repeat;background-position:100%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.6);backdrop-filter:brightness(.6)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:center><h1 class=hero-title>The First Multimodal AI Research Sprint</h1><p class=hero-lead style="margin:0 auto"><p><font size style=color:#fff!important><i>Exploring Multimodal AI: Data and Problem-Solving beyond Vision and Language</i></font><br><font size=3 style=color:#fff!important>In Person on 22nd November 2023, The Alan Turing Institute, London</font><div style=text-align:center><a href=https://twitter.com/MultimodalAI_UK class=btn style=margin:5px;background-color:#fff!important;color:purple!important><i class="fab fa-twitter"></i> Follow Us</a></p></div></p></div></div></div></div><a class=carousel-control-prev href=#slider data-slide=prev><span class=carousel-control-prev-icon></span>
<span class=sr-only>Previous</span>
</a><a class=carousel-control-next href=#slider data-slide=next><span class=carousel-control-next-icon></span>
<span class=sr-only>Next</span></a></section><section id=top class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class=col-12><p>Welcome to &ldquo;<strong>The First Multimodal AI Research Sprint: Beyond Vision & Language</strong>.&rdquo; This unique gathering is where the convergence of diverse data types and cutting-edge machine learning models and algorithms is not just discussed but actively pursued. Our focus is on exploring the untapped potential in Multimodal AI, venturing beyond the established domains of vision and language. This event is a journey into uncharted territories, seeking to uncover new challenges and opportunities in data and problem-solving that have yet to be fully explored. While our primary aim is to delve into these new areas, we also value and welcome the rich insights and contributions from the fields of vision and language. The knowledge from these areas will provide a solid foundation for our exploration and innovation.</p><p>This event is more than a mere meeting of minds; it&rsquo;s the beginning of a long-term collaboration aimed at making a lasting impact in the field of Multimodal AI. It is your expertise, your insights, and your collaborative spirit that will drive us toward achieving significant academic and collaborative milestones.</p></div></div></div></section><section id=programme class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Final Programme (In Person Only)</h1><p>Wednesday, 22nd November 2023</p></div><div class=col-md-12><center><table><thead><tr><th>Time</th><th>    Event</th><th>    Location</th></tr></thead><tbody><tr><td>10:00 - 10:15</td><td>    Welcome and Introduction - Haiping Lu & Xianyuan Liu</td><td>    Enigma</td></tr><tr><td>10:15 - 11:30</td><td>    Pitch Session</td><td>    Enigma</td></tr><tr><td>11:30 - 11:45</td><td>    Coffee break</td><td>    Enigma</td></tr><tr><td>11:45 - 12:15</td><td>    Breakout Session 1</td><td></td></tr><tr><td></td><td>    Healthcare and medicine</td><td>    Enigma</td></tr><tr><td></td><td>    Engineering</td><td>    Margaret Hamilton (12)</td></tr><tr><td></td><td>    Social science and humanities</td><td>    David Blackwell (8)</td></tr><tr><td></td><td>    Science</td><td>    Jack Good (8)</td></tr><tr><td></td><td>    Finance and economics</td><td>    Mae Jemison (7)</td></tr><tr><td></td><td>    Environment and sustainability</td><td>    Cipher (7)</td></tr><tr><td>12:15 - 12:45</td><td>    Breakout reflection and consolidation</td><td>    Enigma</td></tr><tr><td>12:45 - 13:30</td><td>    Lunch (provided)</td><td>    Enigma</td></tr><tr><td>13:30 - 15:30</td><td>    Breakout session 2</td><td></td></tr><tr><td></td><td>    Healthcare and medicine</td><td>    Enigma</td></tr><tr><td></td><td>    Engineering</td><td>    Margaret Hamilton (12)</td></tr><tr><td></td><td>    Social science and humanities</td><td>    David Blackwell (8)</td></tr><tr><td></td><td>    Science</td><td>    Jack Good (8)</td></tr><tr><td></td><td>    Finance and economics</td><td>    Enigma</td></tr><tr><td></td><td>    Environment and sustainability</td><td>    Enigma</td></tr><tr><td>15:30 - 16:00</td><td>    Consolidation and Next Steps</td><td>    Enigma</td></tr></tbody></table></center><center><p style=font-size:24px;font-weight:700>Pitches</p><table><thead><tr><th>Name</th><th>    Title</th></tr></thead><tbody><tr><td>Peter Charlton</td><td>    Using multimodal AI to diagnose atrial fibrillation from smart wearables</td></tr><tr><td>Yuhan Wang</td><td>    Explainable Alzheimer Disease Early Detection Framework Based on Multi-Modal Clinical Data</td></tr><tr><td>Mohammod Suvon</td><td>    Multimodal Cardiothoracic Disease Prediction</td></tr><tr><td>Chris Tomlinson</td><td>    graphICM: graph and semantic representation learning for critical illness aetiology</td></tr><tr><td>Avish Vijayaraghavan</td><td>    Interpretable Multi-Modal Learning for Clinical Multi-Omics</td></tr><tr><td>Luigi Moretti</td><td>    Can MultimodalAI be effectively implemented to help treat Anxiety Disorders?</td></tr><tr><td>Lucas Farndale</td><td>    Super Vision Without Supervision: Self-Supervised Multimodal Privileged Information Integration for Enhanced Biomedical Imaging</td></tr><tr><td>Jinge Wu</td><td>    Facilitating factual checking on radiology reports using multimodal benchmark datasets</td></tr><tr><td>Greg Slabaugh</td><td>    Multimodal AI for Multi&rsquo;omics Data Integration in Healthcare</td></tr><tr><td>Chen Chen</td><td>    Towards Responsible AI in Healthcare: Enhancing Generalizability, Robustness, Explainability, and Fairness with Multi-modality Data</td></tr><tr><td>Marta Varela</td><td>    Physics-Informed Neural Networks</td></tr><tr><td>Oya Celiktutan</td><td>    Multimodal Behavioural AI for Human-Robot Interaction</td></tr><tr><td>Nitisha Jain</td><td>    Semantic Interpretations of Multimodal Embeddings towards Explainable AI</td></tr><tr><td>Roger Moore</td><td>    Vocal interactivity in a multimodal context: pragmatic, synchronic and energetic constraints</td></tr><tr><td>Ruizhe Li</td><td>    Hearing Lips in Noise: Fusing Acoustic and Visual Data for Noise-Robust Speech Recognition</td></tr><tr><td>Cyndie Demeocq</td><td>    Data annotation and curation for multimodal methods in online crime detection systems</td></tr><tr><td>Valentin Danchev</td><td>    Data Governance and Responsible Sharing of Multimodal Data for AI Research</td></tr><tr><td>Lucia Cipolina-Kun</td><td>    Diffusion models for the restoration of cultural heritage</td></tr><tr><td>Pin Ni</td><td>    Financial multi-modal fusion and learning</td></tr><tr><td>Arunav Das</td><td>    Multimodal Knowledge Graph based Question Answering system</td></tr><tr><td>Thijs van der Plas</td><td>    Biodiversity monitoring</td></tr><tr><td>Alejandro Coca-Castro</td><td>    Environmental Data Modalities: Challenges and Opportunities</td></tr></tbody></table></center></div></div></div></section><section id=contact class="home-section wg-contact"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Contact Us</h1></div><div class=col-12><ul class=fa-ul><li><i class="fa-li fas fa-map-marker fa-2x" aria-hidden=true></i>
<span id=person-address>The Alan Turing Institute, 96 Euston Rd, London, NW1 2DB</span></li><li><i class="fa-li fas fa-envelope fa-2x" aria-hidden=true></i>
<a href=mailto:multimodal-ai-enquiry-group@shef.ac.uk>Email the organisers</a></li></ul><div class=d-none><input id=map-provider value=mapnik>
<input id=map-lat value=51.5293529>
<input id=map-lng value=-0.1301265>
<input id=map-dir value="The Alan Turing Institute, 96 Euston Rd, London, NW1 2DB">
<input id=map-zoom value=15>
<input id=map-api-key value></div><div id=map></div></div></div></div></section><section id=direction class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Direction</h1></div><div class=col-12><p><b>By rail</b></p><p>The Institute is located adjacent to St Pancras International Rail Station and within five minute walk of Kings Cross Rail Station and 10 minute walk of Euston Rail Station.</p><p><b>By underground</b></p><p>The nearest underground station is Kings Cross St Pancras Station (five minute walk), which is on the Circle, Hammersmith & City, Metropolitan, Northern (Bank branch), Piccadilly and Victoria underground lines.</p><p><b>By bus</b></p><p>There are many nearby services, including 10, 30, 59, 63, 73, and 91.</p><p><b>Finding the Turing from within the British Library</b></p><p>Enter the British Library via the main entrance on Euston Road.</p><p>To access the first floor via stairs take the main staircase to the left of the ticket and membership desk. Once at the large central bookcase display, turn right and head towards the sign for The Alan Turing Institute.</p><p>To access the first floor via lift go right after entering the British Library, past the ticket and membership desk and fountain, and turn left at the bookshop. Continue until reaching Lift 6. Take the lift to the upper ground floor (UG). Once on the upper ground floor, exit the lift and take one of the three lifts diagonally across to the first floor. On the first floor, exit the lift, turn left, and follow the sign for The Alan Turing Institute.</p><p>The Alan Turing Institute entrance is illuminated with signage over the doorway, behind the display of the enigma machine. Please report to reception once you arrive at the entrance.</p></div></div></div></section><section id=acknowledgement class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class=col-12><center><h1>Acknowledgement</h1></center><p>This event is brought to you by the Turing Interest Group on Meta-Learning for Multimodal Data (welcome to <a href="https://forms.office.com/Pages/ResponsePage.aspx?id=p_SVQ1XklU-Knx-672OE-fR6PcyyBV1JuragBENwKPJUOFhHNkY5WU1RVlczMjNWUVdYTDFDME1VNSQlQCN0PWcu" target=_blank rel=noopener>sign up</a>) and the Multimodal AI Community (welcome to subscribe to our <a href="https://groups.google.com/a/sheffield.ac.uk/g/multimodal-ai-community-group?pli=1" target=_blank rel=noopener>Google Group</a>) supported by the <a href=https://www.sheffield.ac.uk/machine-intelligence target=_blank rel=noopener>Centre for Machine Intelligence</a> at the University of Sheffield.</p></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Multimodal AI Workshop.</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.6ab16275cbca742a586c1726e3d94093.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script></body></html>