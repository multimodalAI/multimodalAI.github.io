<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.b9231f75dcc97a371ce6141b4d2aadaf.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><script async src="https://www.googletagmanager.com/gtag/js?id=G-5K2K2GHWHQ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","G-5K2K2GHWHQ",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><meta name=author content="Haiping Lu"><meta name=description content><link rel=alternate hreflang=en-us href=https://multimodalAI.github.io/multimodalai-forum24/><link rel=canonical href=https://multimodalAI.github.io/multimodalai-forum24/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu25caa61f2e6d310cd2b668efa4781043_104298_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu25caa61f2e6d310cd2b668efa4781043_104298_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@MultimodalAI_UK"><meta property="twitter:creator" content="@MultimodalAI_UK"><meta property="twitter:image" content="https://multimodalAI.github.io/media/icon_hu25caa61f2e6d310cd2b668efa4781043_104298_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="UK Open Multimodal AI Network"><meta property="og:url" content="https://multimodalAI.github.io/multimodalai-forum24/"><meta property="og:title" content="UK Open Multimodal AI Network"><meta property="og:description" content><meta property="og:image" content="https://multimodalAI.github.io/media/icon_hu25caa61f2e6d310cd2b668efa4781043_104298_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><title>UK Open Multimodal AI Network</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=17d477837d4bbd5b51afcf196549e98f><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>UK Open Multimodal AI Network</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>UK Open Multimodal AI Network</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/multimodalai26><span>MultimodalAI'26</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>About</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/#ukomain><span>UKOMAIN</span></a>
<a class=dropdown-item href=/#team><span>Team</span></a>
<a class=dropdown-item href=/#contact><span>Contact</span></a>
<a class=dropdown-item href=/omaib><span>Funding Call</span></a>
<a class=dropdown-item href=/code-of-conduct><span>Code of Conduct</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Resources</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://multimodalai.github.io/multimodal-ai-landscape/><span>Landscape Explorer</span></a>
<a class=dropdown-item href=https://www.youtube.com/@UKMultimodalAI><span>YouTube</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Past Events</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/multimodalai25><span>MultimodalAI'25</span></a>
<a class=dropdown-item href=/multimodalai25/hackathon/><span>MultimodalAI'25 Hackathon</span></a>
<a class=dropdown-item href=/embc25workshop><span>EMBC'25Workshop</span></a>
<a class=dropdown-item href=/multimodalai24><span>MultimodalAI'24</span></a>
<a class=dropdown-item href=/multimodalai-forum24><span>MultimodalAIForum'24</span></a>
<a class=dropdown-item href=/multimodalai-sprint23><span>MultimodalAISprint'23</span></a>
<a class=dropdown-item href=/multimodalai23><span>MultimodalAI'23</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>First Multimodal AI Community Forum</h1></div><div class=col-12><p>Welcome to &ldquo;<strong>The First Multimodal AI Community Forum</strong>&rdquo;, an online <a href=https://ai-uk.turing.ac.uk/fringe-events/>AI UK Fringe</a> event. This event is scheduled from <strong>13:00 to 17:00 (GMT) on Monday, 25th March 2024</strong>. The deadline for <a href=https://forms.gle/yckNWD8kHY5Z1wjo9>registration</a> is <strong>Thursday, 14th March 2024, 23:59 (GMT)</strong>.</p><details><summary style=font-size:24px;border:none>Click to show more details</summary><p style=border:none;margin-left:0>Multimodal AI, which integrates various data modalities such as text, image, sound, and others, is swiftly revolutionising our interaction with technology and data. In our recent Turing Interest Group event (22nd Nov), "<a href=https://multimodalai.github.io/multimodalaisprint23/>The First Multimodal AI Research Sprint</a>", we explored the diverse research states and methodologies in Multimodal AI across six areas and initiated the writing of a perspective paper on multimodal AI. Based on such past activities, this online forum aims to further bring together community members, from researchers to practitioners, to share their latest interdisciplinary perspectives and pioneering work in Multimodal AI. Our goal is to facilitate the exchange of fresh insights and foster connections and research progress within the Multimodal AI community.</p><p style=border:none;margin-left:0>We welcome researchers, practitioners, and students engaged in or interested in Multimodal AI from anywhere in the world to join us online. We also encourage the organisation of local community gatherings to watch and discuss the forum together.</p></details><p style=border:none;margin-left:0>The video recording of the Keynote Presentation by Chunyuan Li is given below.</p><iframe width=560 height=315 src="https://www.youtube.com/embed/6F4fra-QBpk?si=QAhzPnhvdKKT6Qnf&amp;start=1" title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><p style=border:none;margin-left:0>The video recording of the Open Discussion session of the <strong>First Multimodal AI Community Forum</strong> is given below.</p><iframe width=560 height=315 src="https://www.youtube.com/embed/vf6CxlJSuVk?si=-CeaLFU7BDpBt-gt" title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe></div></div></div></section><section id=speaker class="home-section wg-people"><div class=home-section-bg></div><div class=container><div class="row justify-content-center people-widget"><div class="col-md-12 section-heading"><h1>Keynote Speaker</h1></div><div class="col-12 col-sm-auto people-person"><a href=/author/chunyuan-li/><img width=270 height=270 loading=lazy class="avatar avatar-circle" src=/author/chunyuan-li/avatar_huad485b908f71d2416790700550d1daf8_243834_270x270_fill_q75_lanczos_center.jpg alt=Avatar></a><div class=portrait-title><h2><a href=/author/chunyuan-li/>Chunyuan Li</a></h2><h3>Research Lead at ByteDance/TikTok, (Co-)Lead Developer of LLaVA, Former Principal Researcher at Microsoft Research, Redmond</h3></div></div></div></div></section><section id=programme class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Final Programme</h1><p class=mt-1>Monday, 25th March 2024</p></div><div class=col-12><center><table><thead><tr><th>Time (GMT)</th><th>    Event</th></tr></thead><tbody><tr><td>13:00 - 13:10</td><td>    Opening</td></tr><tr><td></td><td>    Haiping Lu - Welcome and Introduction to the Multimodal AI Community</td></tr><tr><td>13:10 - 15:40</td><td>    Pitches</td></tr><tr><td></td><td>    <strong>Open Source Software</strong></td></tr><tr><td></td><td>        Florence Townend - Fusilli: a Python library for comparing deep fusion models</td></tr><tr><td></td><td>        Ana Lawry Aguila - multi-view-AE: a Python library of multi-view autoencoder methods</td></tr><tr><td></td><td>    <strong>Healthcare and Medicine</strong></td></tr><tr><td></td><td>        Jinge Wu - Introduction to Multimodal AI for Healthcare and Medicine</td></tr><tr><td></td><td>        Peter Charlton - Using Multimodal AI to Identify Determinants of Health and Wellbeing</td></tr><tr><td></td><td>        Halimat Afolabi - Med-Image Bind</td></tr><tr><td></td><td>        Dheeraj Giri - AI in Healthcare Needs Multimodal AI for Precision</td></tr><tr><td></td><td>        Farzana Patel - Using Multimodal AI Models to Support Clinical Diagnosis and Assessments</td></tr><tr><td></td><td>        Jinge Wu - Vision-Language Model in Radiology</td></tr><tr><td></td><td>        Jiachen Luo - Multimodal Emotion Recognition, Healthcare</td></tr><tr><td></td><td>        Kerf Tan - Multimodal Consolidation of Health Data, Breaking the Silos, Fast</td></tr><tr><td></td><td>        Anthony Hughes - Causal Graph Discovery Using Multimodal Models</td></tr><tr><td></td><td>    <strong>Q&amp;A and Break</strong></td></tr><tr><td></td><td>    <strong>Social Science and Humanities</strong></td></tr><tr><td></td><td>        Cyndie Demeocq - Introduction to Multimodal AI for Social Science and Humanities</td></tr><tr><td></td><td>        Shashank Shekhar - Unlock Relationships in Multi-Modal Data Using Knowledge Graphs</td></tr><tr><td></td><td>        Shitong Sun - Compositional Multimodal Learning</td></tr><tr><td></td><td>        Masoumeh Chapariniya - Computational Analysis of Personal Identity in Interaction, Recognition and Ethics</td></tr><tr><td></td><td>        Teodora Vukovic - VIAN-DH - Software for Multimodal Corpora Created by Liri In Zurich</td></tr><tr><td></td><td>        Ann Van De Velde - Scientific Illustration and Banned Words in Midjourney and DALLE</td></tr><tr><td></td><td>        Lucia Cipolina-Kun - The Reading of the Herculaneum Papyri Using AI</td></tr><tr><td></td><td>        Shan Wikoon - Agency in Building Near-Human Multimodal AI Teacher</td></tr><tr><td></td><td>    <strong>Q&amp;A and Break</strong></td></tr><tr><td></td><td>    <strong>Engineering</strong></td></tr><tr><td></td><td>        Xianyuan Liu - Introduction to Multimodal AI for Engineering</td></tr><tr><td></td><td>        Ehsan Nowroozi - Validating the Robustness of Cybersecurity Models</td></tr><tr><td></td><td>        Chao Zhang - Multimodal Learning in Embodied Applications</td></tr><tr><td></td><td>        Yao Zhang - AI in Control for Maritime Engineering</td></tr><tr><td></td><td>    <strong>Science</strong></td></tr><tr><td></td><td>        Yuhan Wang - Introduction to Multimodal AI for Science</td></tr><tr><td></td><td>        Ashwath Shetty - Alt-Text Generation for the Images Using Multimodal LLM</td></tr><tr><td></td><td>        Sogol Haghighat - Empowering AI Solutions: HPC Access, Research, and Consulting for Multimodal Models</td></tr><tr><td></td><td>    <strong>Q&amp;A and Break</strong></td></tr><tr><td></td><td>    <strong>Environment and Sustainability</strong></td></tr><tr><td></td><td>        Nataliya Tkachenko - Introduction to Multimodal AI for Environment and Sustainability</td></tr><tr><td></td><td>        Sachin Gaur - Approaches to Understand and Reduce Search Space for Video Data</td></tr><tr><td></td><td>        Natalia Efremova - AI for Agriculture</td></tr><tr><td></td><td>    <strong>Finance and Economics</strong></td></tr><tr><td></td><td>        Arunav Das - Introduction to Multimodal AI for Finance and Economics</td></tr><tr><td></td><td>        Yan Ge - Multimodal Multi-Task Asset Pricing with Token-Level Numeral Learning</td></tr><tr><td></td><td>    <strong>Q&amp;A and Break</strong></td></tr><tr><td>15:40 - 16:20</td><td>    Keynote Presentation by Chunyuan Li</td></tr><tr><td><td style=word-wrap:break-word;max-width:300px;text-align:justify;padding-left:1.3em;text-indent:-1.3em>    <strong>Title:</strong> <a href=https://llava-vl.github.io/ target=_blank rel=noopener>LLaVA: A Vision-Language Approach to Computer Vision in the Wild</a><br><strong>Abstract:</strong> The future of AI is in creating systems like foundation models that are pre-trained once, and will handle countless many downstream tasks directly (zero-shot), or adapt to new tasks quickly (few-shot). In this talk, I will discuss our vision-language approach to achieving “Computer Vision in the Wild (CVinW)”: building such a transferable system in computer vision (CV) that can effortlessly generalize to a wide range of visual recognition tasks in the wild. I will dive into Large Language-and-Vision Assistant (LLaVA) and its series, which is the first open-source project to exhibit the GPT-V level capabilities in image understanding and reasoning. I will demonstrate a promising path to build customizable large multimodal models that follow humans&rsquo; intent with an affordable cost.</td></td><td></td></tr><tr><td>16:20 - 17:00</td><td>    Open Discussion and Conclusions</td></tr></tbody></table></center></div></div></div></section><section id=contact class="home-section wg-contact"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Contact Us</h1></div><div class=col-12><ul class=fa-ul><li><i class="fa-li fas fa-envelope fa-2x" aria-hidden=true></i>
<a href=mailto:multimodal-ai-event-organisers-group@sheffield.ac.uk><a href=mailto:multimodal-ai-event-organisers-group@sheffield.ac.uk>multimodal-ai-event-organisers-group@sheffield.ac.uk</a></a></li></ul></div></div></div></section><section id=acknowledgement class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Acknowledgement</h1></div><div class=col-12><p style=border:none;margin-left:0>This event is brought to you by the Turing Interest Group on <a href=https://www.turing.ac.uk/research/interest-groups/meta-learning-multimodal-data>Meta-Learning for Multimodal Data</a> (welcome to <a href="https://forms.office.com/Pages/ResponsePage.aspx?id=p_SVQ1XklU-Knx-672OE-fR6PcyyBV1JuragBENwKPJUOFhHNkY5WU1RVlczMjNWUVdYTDFDME1VNSQlQCN0PWcu">sign up</a>) and the Multimodal AI Community (welcome to <a href="https://groups.google.com/a/sheffield.ac.uk/g/multimodal-ai-community-group?pli=1">subscribe to our Google Group</a>) supported by the <a href=https://www.sheffield.ac.uk/machine-intelligence>Centre for Machine Intelligence</a> at the University of Sheffield.</p><details><summary style=font-size:24px;border:none>Click to show more details</summary><div style=text-align:center><img src=/media/AIUK_logo.png alt="AIUK Logo" style=width:200px;height:auto;display:inline-block;margin-right:20px>
<img src=/media/CMI_logo_1.png alt="CMI Logo" style=width:420px;height:auto;display:inline-block;margin-right:20px></div></details></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2026 UK Open Multimodal AI Network (UKOMAIN).<br><a href=/privacy-and-data-use/>Privacy and Data Use Notice</a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.6ab16275cbca742a586c1726e3d94093.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script></body></html>